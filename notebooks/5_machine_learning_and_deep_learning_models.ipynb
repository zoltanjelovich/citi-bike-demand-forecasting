{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994226af-ec22-436f-86c8-051c50ee5718",
   "metadata": {},
   "source": [
    "# Machine learning and deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb285ccd-aa02-461d-bd3d-db3ba2d9b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# Standard library\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Optional, Tuple\n",
    "\n",
    "# Third-party libraries\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ef035-aa4a-46dd-be23-aa5430c01a09",
   "metadata": {},
   "source": [
    "## Common data partitioning\n",
    "\n",
    "To ensure comparability across models, consistent data partitions were used at each modeling level:\n",
    "\n",
    "* The same train, validation, and test segments were applied to all models:\n",
    "\n",
    "  * Training period: 2017–2021\n",
    "  * Validation period: 2022\n",
    "  * Test period: 2023–2024\n",
    "\n",
    "* For hourly models, temporal ordering was preserved within each split, and rolling-origin evaluation was used to improve robustness:\n",
    "\n",
    "  * Models were trained on data spanning [start … t] and one-step-ahead forecasts were generated for t + 1, iterating through the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73dddb14-7d0f-4652-b6d3-ba431f783375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common data partitioning\n",
    "\n",
    "BASE_DIR = \"/Users/zoltanjelovich/Documents/ISEG/MFW/data\"\n",
    "\n",
    "# Targets\n",
    "PATH_Y_DAILY_SYSTEM  = f\"{BASE_DIR}/targets/system/y_daily_system.parquet\"\n",
    "PATH_Y_HOURLY_SYSTEM = f\"{BASE_DIR}/targets/system/y_hourly_system.parquet\"\n",
    "PATH_Y_HOURLY_STATION = f\"{BASE_DIR}/targets/station/y_hourly_station.parquet\"\n",
    "\n",
    "# Partition windows\n",
    "TRAIN_START = pd.Timestamp(\"2017-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2021-12-31 23:00:00\")\n",
    "VAL_START   = pd.Timestamp(\"2022-01-01\")\n",
    "VAL_END     = pd.Timestamp(\"2022-12-31 23:00:00\")\n",
    "TEST_START  = pd.Timestamp(\"2023-01-01\")\n",
    "TEST_END    = pd.Timestamp(\"2024-12-31 23:00:00\")\n",
    "\n",
    "def _to_ts(x) -> pd.Timestamp:\n",
    "    return pd.Timestamp(x)\n",
    "\n",
    "def add_split_label(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[time_col] = pd.to_datetime(out[time_col])\n",
    "\n",
    "    t = out[time_col]\n",
    "    split = np.where((t >= TRAIN_START) & (t <= TRAIN_END), \"train\",\n",
    "             np.where((t >= VAL_START)   & (t <= VAL_END),   \"val\",\n",
    "             np.where((t >= TEST_START)  & (t <= TEST_END),  \"test\", None)))\n",
    "\n",
    "    out[\"split\"] = split\n",
    "    out = out[out[\"split\"].notna()].copy()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab125a3-0a46-4a74-b587-c6fb3eb5da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    1826\n",
      "test      731\n",
      "val       365\n",
      "Name: count, dtype: int64\n",
      "2017-01-01 00:00:00 → 2024-12-31 00:00:00\n",
      "split\n",
      "train    43824\n",
      "test     17544\n",
      "val       8760\n",
      "Name: count, dtype: int64\n",
      "2017-01-01 00:00:00 → 2024-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Apply to system-level targets (daily + hourly)\n",
    "\n",
    "# Daily system target\n",
    "y_daily_system = pd.read_parquet(PATH_Y_DAILY_SYSTEM)\n",
    "y_daily_system = add_split_label(y_daily_system, time_col=\"date\")\n",
    "print(y_daily_system[\"split\"].value_counts(dropna=False))\n",
    "print(y_daily_system[\"date\"].min(), \"→\", y_daily_system[\"date\"].max())\n",
    "\n",
    "# Hourly system target\n",
    "y_hourly_system = pd.read_parquet(PATH_Y_HOURLY_SYSTEM)\n",
    "y_hourly_system = add_split_label(y_hourly_system, time_col=\"ts_hour\")\n",
    "y_hourly_system = y_hourly_system.sort_values(\"ts_hour\")\n",
    "print(y_hourly_system[\"split\"].value_counts(dropna=False))\n",
    "print(y_hourly_system[\"ts_hour\"].min(), \"→\", y_hourly_system[\"ts_hour\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf865d32-a378-4b63-a046-9d0c2ce99c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "test     22157137\n",
      "train    21764585\n",
      "val       8255194\n",
      "Name: count, dtype: int64\n",
      "2017-01-01 00:00:00 → 2024-12-31 23:00:00\n",
      "Unique stations: 3594\n"
     ]
    }
   ],
   "source": [
    "# Apply to station-level hourly target\n",
    "\n",
    "y_hourly_station = pd.read_parquet(PATH_Y_HOURLY_STATION, columns=[\"ts_hour\", \"station_id\", \"departures\", \"arrivals\", \"net\"])\n",
    "y_hourly_station = add_split_label(y_hourly_station, time_col=\"ts_hour\")\n",
    "y_hourly_station = y_hourly_station.sort_values([\"ts_hour\", \"station_id\"])\n",
    "\n",
    "print(y_hourly_station[\"split\"].value_counts())\n",
    "print(y_hourly_station[\"ts_hour\"].min(), \"→\", y_hourly_station[\"ts_hour\"].max())\n",
    "print(\"Unique stations:\", y_hourly_station[\"station_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656ff702-3b51-427b-8368-ba42114b40fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RollingOriginStep(train_end=Timestamp('2022-12-31 23:00:00'), pred_time=Timestamp('2023-01-01 00:00:00')),\n",
       " RollingOriginStep(train_end=Timestamp('2023-01-01 00:00:00'), pred_time=Timestamp('2023-01-01 01:00:00')),\n",
       " RollingOriginStep(train_end=Timestamp('2023-01-01 01:00:00'), pred_time=Timestamp('2023-01-01 02:00:00')),\n",
       " RollingOriginStep(train_end=Timestamp('2023-01-01 02:00:00'), pred_time=Timestamp('2023-01-01 03:00:00')),\n",
       " RollingOriginStep(train_end=Timestamp('2023-01-01 03:00:00'), pred_time=Timestamp('2023-01-01 04:00:00'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rolling-origin “expanding window” iterator (forecast t+1)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RollingOriginStep:\n",
    "    train_end: pd.Timestamp\n",
    "    pred_time: pd.Timestamp\n",
    "\n",
    "def rolling_origin_hours(\n",
    "    test_start: pd.Timestamp,\n",
    "    test_end: pd.Timestamp,\n",
    "    train_start: pd.Timestamp = TRAIN_START,\n",
    "    min_train_end: pd.Timestamp = TRAIN_END,\n",
    "    step_hours: int = 1,\n",
    ") -> Iterator[RollingOriginStep]:\n",
    "    # First prediction time is the first hour in test period\n",
    "    pred_time = test_start\n",
    "\n",
    "    # Corresponding train_end is one hour before the prediction\n",
    "    train_end = pred_time - pd.Timedelta(hours=1)\n",
    "\n",
    "    # Enforce minimum training end\n",
    "    if train_end < min_train_end:\n",
    "        train_end = min_train_end\n",
    "        pred_time = train_end + pd.Timedelta(hours=1)\n",
    "\n",
    "    while pred_time <= test_end:\n",
    "        yield RollingOriginStep(train_end=train_end, pred_time=pred_time)\n",
    "        pred_time = pred_time + pd.Timedelta(hours=step_hours)\n",
    "        train_end = pred_time - pd.Timedelta(hours=1)\n",
    "\n",
    "# Preview the first 5 steps\n",
    "steps = list(itertools.islice(rolling_origin_hours(TEST_START, TEST_END), 5))\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e40c35-5de8-4269-8771-5a053ad6bd5d",
   "metadata": {},
   "source": [
    "## Feature sets per level\n",
    "\n",
    "Consistent feature families were used across modeling levels:\n",
    "\n",
    "* For system-level machine learning models:\n",
    "\n",
    "  * Target variables included `y_hourly_system` and `y_daily_system` for one-step-ahead forecasting.\n",
    "  * Feature sets included time-of-day and time-of-week indicators, weather variables, air quality index (AQI) measures, and phase indicators.\n",
    "\n",
    "* For station-level machine learning and deep learning models:\n",
    "\n",
    "  * The primary target variable was `y_hourly_station`, with log transformation applied where appropriate.\n",
    "  * Feature sets included:\n",
    "\n",
    "    * Temporal features such as hour, day-of-week, seasonality, and holiday indicators.\n",
    "    * Weather variables.\n",
    "    * Built environment and demographic attributes represented as station-level static features.\n",
    "    * Network features including centrality measures, degree, and cluster membership.\n",
    "    * Contextual variables such as traffic measures and bicycle counts.\n",
    "\n",
    "* For cluster-level models:\n",
    "\n",
    "  * Feature sets followed the station-level structure but were aggregated at the cluster level, enabling simpler model specifications and direct comparison with station-level results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97842c3-3be2-417f-804b-5af2fda18c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly system shapes: (70127, 84) (70127,) (70127, 2)\n",
      "split\n",
      "train    43823\n",
      "test     17544\n",
      "val       8760\n",
      "Name: count, dtype: int64\n",
      "Pred-time range: 2017-01-01 01:00:00 → 2024-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Hourly system\n",
    "\n",
    "BASE_DIR = \"/Users/zoltanjelovich/Documents/ISEG/MFW/data\"\n",
    "PATH_X_HOURLY_SYSTEM = f\"{BASE_DIR}/features/system/X_hourly_system.parquet\"\n",
    "\n",
    "# Split boundaries\n",
    "TRAIN_START = pd.Timestamp(\"2017-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2021-12-31 23:00:00\")\n",
    "VAL_START   = pd.Timestamp(\"2022-01-01\")\n",
    "VAL_END     = pd.Timestamp(\"2022-12-31 23:00:00\")\n",
    "TEST_START  = pd.Timestamp(\"2023-01-01\")\n",
    "TEST_END    = pd.Timestamp(\"2024-12-31 23:00:00\")\n",
    "\n",
    "def add_split_label_on_time(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    t = pd.to_datetime(out[time_col])\n",
    "    out[time_col] = t\n",
    "\n",
    "    split = np.where((t >= TRAIN_START) & (t <= TRAIN_END), \"train\",\n",
    "             np.where((t >= VAL_START)   & (t <= VAL_END),   \"val\",\n",
    "             np.where((t >= TEST_START)  & (t <= TEST_END),  \"test\", None)))\n",
    "\n",
    "    out[\"split\"] = split\n",
    "    return out[out[\"split\"].notna()].copy()\n",
    "\n",
    "# Read hourly system features\n",
    "df = pd.read_parquet(PATH_X_HOURLY_SYSTEM)\n",
    "df[\"ts_hour\"] = pd.to_datetime(df[\"ts_hour\"])\n",
    "df = df.sort_values(\"ts_hour\").reset_index(drop=True)\n",
    "\n",
    "# Create 1-step-ahead target and prediction timestamp\n",
    "df[\"pred_time\"] = df[\"ts_hour\"] + pd.Timedelta(hours=1)\n",
    "df[\"y\"] = df[\"trips\"].shift(-1)\n",
    "\n",
    "# Drop last row (no t+1 target) and any rows where y is missing\n",
    "df = df.dropna(subset=[\"y\"]).copy()\n",
    "df[\"y\"] = df[\"y\"].astype(np.float64)\n",
    "\n",
    "# Assign split based on pred_time (target time)\n",
    "df = add_split_label_on_time(df, time_col=\"pred_time\")\n",
    "\n",
    "# Define feature columns\n",
    "drop_cols = {\"ts_hour\", \"pred_time\", \"y\", \"trips\"}\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# Basic preprocessing: one-hot encode object columns, cast bools to int\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"y\"].copy()\n",
    "meta = df[[\"pred_time\", \"split\"]].copy()\n",
    "\n",
    "obj_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "if obj_cols:\n",
    "    X = pd.get_dummies(X, columns=obj_cols, drop_first=False)\n",
    "\n",
    "bool_cols = X.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "for c in bool_cols:\n",
    "    X[c] = X[c].astype(np.int8)\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Hourly system shapes:\", X.shape, y.shape, meta.shape)\n",
    "print(meta[\"split\"].value_counts())\n",
    "print(\"Pred-time range:\", meta[\"pred_time\"].min(), \"→\", meta[\"pred_time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921b4fb2-2bad-473e-a856-e5f08051c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily system shapes: (2921, 77) (2921,) (2921, 2)\n",
      "split\n",
      "train    1825\n",
      "test      731\n",
      "val       365\n",
      "Name: count, dtype: int64\n",
      "Pred-date range: 2017-01-02 00:00:00 → 2024-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Daily system (features at t, trips at t+1 day)\n",
    "\n",
    "PATH_X_DAILY_SYSTEM = f\"{BASE_DIR}/features/system/X_daily_system.parquet\"\n",
    "\n",
    "dfd = pd.read_parquet(PATH_X_DAILY_SYSTEM)\n",
    "dfd[\"date\"] = pd.to_datetime(dfd[\"date\"])\n",
    "dfd = dfd.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "dfd[\"pred_date\"] = dfd[\"date\"] + pd.Timedelta(days=1)\n",
    "dfd[\"y\"] = dfd[\"trips\"].shift(-1)\n",
    "\n",
    "dfd = dfd.dropna(subset=[\"y\"]).copy()\n",
    "dfd[\"y\"] = dfd[\"y\"].astype(np.float64)\n",
    "\n",
    "# Split by target date (pred_date)\n",
    "dfd = add_split_label_on_time(dfd, time_col=\"pred_date\")\n",
    "\n",
    "drop_cols = {\"date\", \"pred_date\", \"y\", \"trips\"}\n",
    "feature_cols = [c for c in dfd.columns if c not in drop_cols]\n",
    "\n",
    "Xd = dfd[feature_cols].copy()\n",
    "yd = dfd[\"y\"].copy()\n",
    "meta_d = dfd[[\"pred_date\", \"split\"]].copy()\n",
    "\n",
    "obj_cols = Xd.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "if obj_cols:\n",
    "    Xd = pd.get_dummies(Xd, columns=obj_cols, drop_first=False)\n",
    "\n",
    "bool_cols = Xd.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "for c in bool_cols:\n",
    "    Xd[c] = Xd[c].astype(np.int8)\n",
    "\n",
    "print(\"Daily system shapes:\", Xd.shape, yd.shape, meta_d.shape)\n",
    "print(meta_d[\"split\"].value_counts())\n",
    "print(\"Pred-date range:\", meta_d[\"pred_date\"].min(), \"→\", meta_d[\"pred_date\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05eda2a6-174e-47ec-a01b-be906899678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a supervised station-level dataset (departures at t+1)\n",
    "\n",
    "# BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "# X_DIR = BASE_DIR / \"features\" / \"station\" / \"hourly\"\n",
    "# OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_supervised_yearly_polars\"\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# years = range(2017, 2025)\n",
    "\n",
    "# for y in years:\n",
    "#     src = X_DIR / f\"X_hourly_station_{y}.parquet\"\n",
    "#     assert src.exists(), f\"Missing: {src}\"\n",
    "\n",
    "#     out_path = OUT_DIR / f\"station_hourly_supervised_{y}.parquet\"\n",
    "\n",
    "#     lf = pl.scan_parquet(str(src))\n",
    "#     lf2 = (\n",
    "#         lf\n",
    "#         .sort([\"station_id\", \"ts_hour\"])\n",
    "#         .with_columns([\n",
    "#             (pl.col(\"ts_hour\") + pl.duration(hours=1)).alias(\"pred_time\"),\n",
    "#             pl.col(\"departures\").shift(-1).over(\"station_id\").alias(\"y\"),\n",
    "#         ])\n",
    "#         .filter(pl.col(\"y\").is_not_null())\n",
    "#         .with_columns([\n",
    "#             pl.when(pl.col(\"pred_time\") < pl.datetime(2022, 1, 1, 0, 0, 0))\n",
    "#               .then(pl.lit(\"train\"))\n",
    "#               .when(pl.col(\"pred_time\") < pl.datetime(2023, 1, 1, 0, 0, 0))\n",
    "#               .then(pl.lit(\"val\"))\n",
    "#               .otherwise(pl.lit(\"test\"))\n",
    "#               .alias(\"split\")\n",
    "#         ])\n",
    "#     )\n",
    "\n",
    "#     # Stream directly to parquet (no full in-memory collect)\n",
    "#     lf2.sink_parquet(str(out_path))\n",
    "#     print(f\"Wrote {out_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddaeed08-357d-4f49-ac6d-31934f235ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "┌───────┬──────────┐\n",
      "│ split ┆ n        │\n",
      "│ ---   ┆ ---      │\n",
      "│ str   ┆ u32      │\n",
      "╞═══════╪══════════╡\n",
      "│ test  ┆ 41653875 │\n",
      "│ train ┆ 48151826 │\n",
      "│ val   ┆ 16046488 │\n",
      "└───────┴──────────┘\n",
      "shape: (1, 2)\n",
      "┌─────────────────────┬─────────────────────┐\n",
      "│ min_pred            ┆ max_pred            │\n",
      "│ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ datetime[μs]        │\n",
      "╞═════════════════════╪═════════════════════╡\n",
      "│ 2017-01-01 01:00:00 ┆ 2024-12-31 23:00:00 │\n",
      "└─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Verification (counts + min/max pred_time)\n",
    "\n",
    "BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_supervised_yearly\"\n",
    "pattern = str(OUT_DIR / \"station_hourly_supervised_*.parquet\")\n",
    "\n",
    "lf = pl.scan_parquet(pattern)\n",
    "\n",
    "counts = lf.group_by(\"split\").agg(pl.len().alias(\"n\")).sort(\"split\").collect()\n",
    "rng = lf.select([\n",
    "    pl.col(\"pred_time\").min().alias(\"min_pred\"),\n",
    "    pl.col(\"pred_time\").max().alias(\"max_pred\"),\n",
    "]).collect()\n",
    "\n",
    "print(counts)\n",
    "print(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0a82dc-92ab-4011-abe0-d004621dbe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String columns: ['station_id', 'holiday_short', 'holiday_name', 'pandemic_regime', 'station_name', 'borough_name', 'nta_code', 'nta_name', 'tract_geoid', 'tract_name', 'state_fips', 'county_fips', 'tract_code', 'county_name', 'pluto_zipcode', 'pluto_landuse_label', 'aqi_category', 'aqi_defining_parameter', 'split']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('station_id', 115),\n",
       " ('station_name', 115),\n",
       " ('tract_geoid', 93),\n",
       " ('tract_name', 90),\n",
       " ('tract_code', 83),\n",
       " ('pluto_zipcode', 30),\n",
       " ('nta_code', 27),\n",
       " ('nta_name', 27),\n",
       " ('holiday_short', 11),\n",
       " ('holiday_name', 11),\n",
       " ('pluto_landuse_label', 11),\n",
       " ('aqi_category', 4),\n",
       " ('borough_name', 3),\n",
       " ('county_fips', 3),\n",
       " ('county_name', 3),\n",
       " ('aqi_defining_parameter', 3),\n",
       " ('state_fips', 2),\n",
       " ('pandemic_regime', 1),\n",
       " ('split', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cardinalities on a bounded sample\n",
    "\n",
    "BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "IN_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_supervised_yearly\"\n",
    "pattern = str(IN_DIR / \"station_hourly_supervised_*.parquet\")\n",
    "\n",
    "lf = pl.scan_parquet(pattern)\n",
    "\n",
    "schema = lf.collect_schema()\n",
    "string_cols = [c for c, dt in schema.items() if dt == pl.Utf8]\n",
    "\n",
    "print(\"String columns:\", string_cols)\n",
    "\n",
    "SAMPLE_N = 1_000_000\n",
    "sample_lf = lf.head(SAMPLE_N)\n",
    "\n",
    "card_df = (\n",
    "    sample_lf.select([pl.col(c).n_unique().alias(c) for c in string_cols])\n",
    "    .collect()\n",
    ")\n",
    "card = card_df.to_dicts()[0]\n",
    "card_sorted = sorted(card.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "card_sorted[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f551ed-92dc-4b6f-ae90-0f44706edbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write ML-ready datasets\n",
    "\n",
    "# BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "# IN_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_supervised_yearly_polars\"\n",
    "# OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml\"\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# pattern = str(IN_DIR / \"station_hourly_supervised_*.parquet\")\n",
    "# lf = pl.scan_parquet(pattern)\n",
    "# schema = lf.collect_schema()\n",
    "\n",
    "# META_COLS = {\"ts_hour\", \"pred_time\", \"split\", \"station_id\"}\n",
    "# TARGET_COLS = {\"y\", \"departures\", \"arrivals\", \"net\"}\n",
    "\n",
    "# DROP_STRING_COLS = {\n",
    "#     \"station_name\",\n",
    "#     \"tract_geoid\", \"tract_name\", \"tract_code\",\n",
    "#     \"nta_code\", \"nta_name\",\n",
    "#     \"pluto_zipcode\",\n",
    "#     \"state_fips\", \"county_fips\", \"county_name\",\n",
    "# }\n",
    "\n",
    "# KEEP_STRING_COLS = {\n",
    "#     \"holiday_short\",\n",
    "#     \"holiday_name\",\n",
    "#     \"pluto_landuse_label\",\n",
    "#     \"aqi_category\",\n",
    "#     \"aqi_defining_parameter\",\n",
    "#     \"borough_name\",\n",
    "#     \"pandemic_regime\",\n",
    "# }\n",
    "\n",
    "# # Build feature column list\n",
    "\n",
    "# feature_cols = []\n",
    "# for c, dt in schema.items():\n",
    "#     if c in META_COLS or c in TARGET_COLS:\n",
    "#         continue\n",
    "#     if dt == pl.Utf8 and c not in KEEP_STRING_COLS:\n",
    "#         continue\n",
    "#     feature_cols.append(c)\n",
    "\n",
    "# print(f\"Number of ML features: {len(feature_cols)}\")\n",
    "\n",
    "# # Prepare base frame (meta + y + features)\n",
    "\n",
    "# base_lf = lf.select(\n",
    "#     [\"station_id\", \"pred_time\", \"split\", \"y\"] + feature_cols\n",
    "# )\n",
    "\n",
    "# # Cast booleans to int8\n",
    "# bool_cols = [c for c in feature_cols if schema.get(c) == pl.Boolean]\n",
    "# if bool_cols:\n",
    "#     base_lf = base_lf.with_columns(\n",
    "#         [pl.col(c).cast(pl.Int8) for c in bool_cols]\n",
    "#     )\n",
    "\n",
    "# # Write split-specific datasets\n",
    "\n",
    "# for split in [\"train\", \"val\", \"test\"]:\n",
    "#     out_path = OUT_DIR / f\"{split}.parquet\"\n",
    "#     (\n",
    "#         base_lf\n",
    "#         .filter(pl.col(\"split\") == split)\n",
    "#         .drop(\"split\")\n",
    "#         .sink_parquet(str(out_path))\n",
    "#     )\n",
    "#     print(f\"Wrote {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e210c08-7e86-41d3-a23d-26a193fad677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "shape: (1, 1)\n",
      "┌──────────┐\n",
      "│ len      │\n",
      "│ ---      │\n",
      "│ u32      │\n",
      "╞══════════╡\n",
      "│ 48151826 │\n",
      "└──────────┘\n",
      "Columns: 129\n",
      "\n",
      "VAL\n",
      "shape: (1, 1)\n",
      "┌──────────┐\n",
      "│ len      │\n",
      "│ ---      │\n",
      "│ u32      │\n",
      "╞══════════╡\n",
      "│ 16046488 │\n",
      "└──────────┘\n",
      "Columns: 129\n",
      "\n",
      "TEST\n",
      "shape: (1, 1)\n",
      "┌──────────┐\n",
      "│ len      │\n",
      "│ ---      │\n",
      "│ u32      │\n",
      "╞══════════╡\n",
      "│ 41653875 │\n",
      "└──────────┘\n",
      "Columns: 129\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml\"\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    lf = pl.scan_parquet(str(OUT_DIR / f\"{split}.parquet\"))\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    print(lf.select(pl.len()).collect())\n",
    "    print(\"Columns:\", len(lf.collect_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9950583e-9d8b-4308-ab8f-bb19fec3ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify traffic columns and confirm substitutes exist\n",
    "\n",
    "# BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "# IN_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml\"\n",
    "# OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml_v2_no_traffic\"\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Scan schema from train split (schemas are identical across splits)\n",
    "# schema = pl.scan_parquet(str(IN_DIR / \"train.parquet\")).collect_schema()\n",
    "# cols = schema.names()\n",
    "\n",
    "# traffic_cols = [c for c in cols if c.startswith(\"traffic_volume_\")]\n",
    "\n",
    "# # Functional substitutes\n",
    "# substitute_candidates = [\n",
    "#     \"bike_count_nearest_counter\",\n",
    "#     # PLUTO / built environment intensity (keep those that exist)\n",
    "#     \"pluto_pct_residential\",\n",
    "#     \"pluto_pct_commercial\",\n",
    "#     \"pluto_far\",\n",
    "#     \"pluto_units_density_per_10k_sqft\",\n",
    "#     \"nearest_pluto_distance_m\",\n",
    "#     # Land-use mix proxies (keep those that exist)\n",
    "#     \"landuse_entropy\",\n",
    "#     \"residential_share\",\n",
    "#     \"commercial_share\",\n",
    "#     \"industrial_share\",\n",
    "#     \"institutional_share\",\n",
    "#     # ACS contextual proxies (keep those that exist)\n",
    "#     \"acs_population\",\n",
    "#     \"acs_pop_density\",\n",
    "#     \"acs_median_income\",\n",
    "#     \"acs_pct_bike_commute\",\n",
    "#     \"acs_pct_transit_commute\",\n",
    "#     \"acs_pct_no_vehicle\",\n",
    "#     \"acs_pct_bachelors_plus\",\n",
    "# ]\n",
    "\n",
    "# present_substitutes = [c for c in substitute_candidates if c in cols]\n",
    "\n",
    "# print(\"Traffic cols to drop:\", traffic_cols)\n",
    "# print(\"Present substitutes:\", present_substitutes)\n",
    "# print(\"N traffic cols:\", len(traffic_cols))\n",
    "# print(\"N present substitutes:\", len(present_substitutes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c47091d9-012a-491c-9a98-6742c9bae41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write updated split files\n",
    "\n",
    "# BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "# IN_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml\"\n",
    "# OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml_v2_no_traffic\"\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Determine columns to drop from schema\n",
    "# schema = pl.scan_parquet(str(IN_DIR / \"train.parquet\")).collect_schema()\n",
    "# cols = schema.names()\n",
    "# traffic_cols = [c for c in cols if c.startswith(\"traffic_volume_\")]\n",
    "\n",
    "# assert len(traffic_cols) > 0, \"No traffic_volume_* columns found to drop.\"\n",
    "\n",
    "# for split in [\"train\", \"val\", \"test\"]:\n",
    "#     in_path = IN_DIR / f\"{split}.parquet\"\n",
    "#     out_path = OUT_DIR / f\"{split}.parquet\"\n",
    "\n",
    "#     lf = pl.scan_parquet(str(in_path)).drop(traffic_cols)\n",
    "#     lf.sink_parquet(str(out_path))\n",
    "#     print(f\"Wrote {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364e838b-b7db-4e79-bae1-ab45e2769e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any traffic_volume_* remaining? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/tx5m_rsx42d8dp9md2z_by1m0000gn/T/ipykernel_18809/3304440927.py:18: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  for c in sample.columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bike_count_nearest_counter', 0.350524),\n",
       " ('pluto_pct_residential', 0.148903),\n",
       " ('pluto_pct_commercial', 0.148903),\n",
       " ('aqi_county', 0.081977),\n",
       " ('aqi_category', 0.081977),\n",
       " ('aqi_defining_parameter', 0.081977),\n",
       " ('aqi_county_usg_or_worse', 0.081977),\n",
       " ('aqi_county_unhealthy_or_worse', 0.081977),\n",
       " ('acs_population', 0.061313),\n",
       " ('acs_median_income', 0.061313),\n",
       " ('acs_pct_bike_commute', 0.061313),\n",
       " ('acs_pct_transit_commute', 0.061313),\n",
       " ('acs_pct_no_vehicle', 0.061313),\n",
       " ('acs_pct_bachelors_plus', 0.061313),\n",
       " ('acs_bike_commute_share', 0.061313)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm traffic cols are gone + substitutes remain + missingness improved\n",
    "\n",
    "BASE_DIR = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "OUT_DIR = BASE_DIR / \"modeling\" / \"station\" / \"hourly_ml\"\n",
    "\n",
    "# Confirm columns removed\n",
    "schema2 = pl.scan_parquet(str(OUT_DIR / \"train.parquet\")).collect_schema()\n",
    "cols2 = schema2.names()\n",
    "print(\"Any traffic_volume_* remaining?\", any(c.startswith(\"traffic_volume_\") for c in cols2))\n",
    "\n",
    "# Re-run missingness on a sample to confirm the \"worst offenders\" changed\n",
    "SAMPLE_N = 1_000_000\n",
    "sample = pl.scan_parquet(str(OUT_DIR / \"train.parquet\")).head(SAMPLE_N)\n",
    "\n",
    "null_rates = (\n",
    "    sample.select([\n",
    "        (pl.col(c).null_count() / SAMPLE_N).alias(c)\n",
    "        for c in sample.columns\n",
    "        if c not in {\"station_id\", \"pred_time\", \"y\"}\n",
    "    ])\n",
    "    .collect()\n",
    "    .to_dicts()[0]\n",
    ")\n",
    "\n",
    "worst = sorted(null_rates.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076c47e0-d183-43b5-bba8-921111d0d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly cluster rows by split:\n",
      "shape: (3, 2)\n",
      "┌───────┬────────┐\n",
      "│ split ┆ n      │\n",
      "│ ---   ┆ ---    │\n",
      "│ str   ┆ u32    │\n",
      "╞═══════╪════════╡\n",
      "│ train ┆ 108369 │\n",
      "│ val   ┆ 26277  │\n",
      "│ test  ┆ 52626  │\n",
      "└───────┴────────┘\n",
      "\n",
      "Hourly pred_time range:\n",
      "shape: (1, 2)\n",
      "┌─────────────────────┬─────────────────────┐\n",
      "│ min_pred            ┆ max_pred            │\n",
      "│ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ datetime[μs]        │\n",
      "╞═════════════════════╪═════════════════════╡\n",
      "│ 2017-01-01 01:00:00 ┆ 2021-12-31 23:00:00 │\n",
      "└─────────────────────┴─────────────────────┘\n",
      "\n",
      "Daily cluster rows by split:\n",
      "shape: (3, 2)\n",
      "┌───────┬──────┐\n",
      "│ split ┆ n    │\n",
      "│ ---   ┆ ---  │\n",
      "│ str   ┆ u32  │\n",
      "╞═══════╪══════╡\n",
      "│ train ┆ 4545 │\n",
      "│ val   ┆ 1095 │\n",
      "│ test  ┆ 2193 │\n",
      "└───────┴──────┘\n",
      "\n",
      "Daily pred_date range:\n",
      "shape: (1, 2)\n",
      "┌─────────────────────┬─────────────────────┐\n",
      "│ min_pred            ┆ max_pred            │\n",
      "│ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ datetime[μs]        │\n",
      "╞═════════════════════╪═════════════════════╡\n",
      "│ 2017-01-02 00:00:00 ┆ 2021-12-31 00:00:00 │\n",
      "└─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "\n",
    "XH_PATH = BASE / \"features/cluster/X_hourly_cluster.parquet\"\n",
    "XD_PATH = BASE / \"features/cluster/X_daily_cluster.parquet\"\n",
    "\n",
    "OUT_H = BASE / \"modeling/cluster/hourly_ml\"\n",
    "OUT_D = BASE / \"modeling/cluster/daily_ml\"\n",
    "OUT_H.mkdir(parents=True, exist_ok=True)\n",
    "OUT_D.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Same split boundaries\n",
    "TRAIN_END_H = pl.datetime(2021, 12, 31, 23, 0, 0)\n",
    "VAL_END_H   = pl.datetime(2022, 12, 31, 23, 0, 0)\n",
    "\n",
    "TRAIN_END_D = pl.datetime(2021, 12, 31, 0, 0, 0)\n",
    "VAL_END_D   = pl.datetime(2022, 12, 31, 0, 0, 0)\n",
    "\n",
    "def add_split_hourly(pred_time_expr: pl.Expr) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(pred_time_expr <= TRAIN_END_H).then(pl.lit(\"train\"))\n",
    "         .when(pred_time_expr <= VAL_END_H).then(pl.lit(\"val\"))\n",
    "         .otherwise(pl.lit(\"test\"))\n",
    "         .alias(\"split\")\n",
    "    )\n",
    "\n",
    "def add_split_daily(pred_date_expr: pl.Expr) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(pred_date_expr <= TRAIN_END_D).then(pl.lit(\"train\"))\n",
    "         .when(pred_date_expr <= VAL_END_D).then(pl.lit(\"val\"))\n",
    "         .otherwise(pl.lit(\"test\"))\n",
    "         .alias(\"split\")\n",
    "    )\n",
    "\n",
    "# # Hourly cluster supervised\n",
    "# lf_h = pl.scan_parquet(str(XH_PATH))\n",
    "\n",
    "# # Exclude identifiers/time and leakage columns from ML feature set.\n",
    "# ID_TIME_COLS_H = {\"ts_hour\", \"cluster_id\", \"cluster_label\", \"date\", \"year\", \"month\", \"day\", \"hour\", \"dow\"}\n",
    "# DROP_FROM_X_H = {\"departures\", \"arrivals\", \"net\"}  # contemporaneous values at ts_hour (avoid trivial leakage)\n",
    "\n",
    "# schema_h = lf_h.collect_schema()\n",
    "# all_cols_h = schema_h.names()\n",
    "\n",
    "# ml_features_h = [\n",
    "#     c for c in all_cols_h\n",
    "#     if (c not in ID_TIME_COLS_H) and (c not in DROP_FROM_X_H)\n",
    "# ]\n",
    "\n",
    "# hourly_supervised = (\n",
    "#     lf_h\n",
    "#     .sort([\"cluster_id\", \"ts_hour\"])\n",
    "#     .with_columns([\n",
    "#         # 1-step-ahead target within cluster\n",
    "#         pl.col(\"departures\").shift(-1).over(\"cluster_id\").alias(\"y\"),\n",
    "#         (pl.col(\"ts_hour\") + pl.duration(hours=1)).alias(\"pred_time\"),\n",
    "#     ])\n",
    "#     .with_columns([\n",
    "#         add_split_hourly(pl.col(\"pred_time\")),\n",
    "#     ])\n",
    "#     .filter(pl.col(\"y\").is_not_null())\n",
    "#     .select(\n",
    "#         [\"pred_time\", \"cluster_id\", \"cluster_label\", \"y\", \"split\"] + ml_features_h\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Write split files\n",
    "# for s in [\"train\", \"val\", \"test\"]:\n",
    "#     out_path = OUT_H / f\"{s}.parquet\"\n",
    "#     hourly_supervised.filter(pl.col(\"split\") == s).sink_parquet(str(out_path))\n",
    "#     print(\"Wrote\", out_path)\n",
    "\n",
    "# # Daily cluster supervised\n",
    "# lf_d = pl.scan_parquet(str(XD_PATH))\n",
    "\n",
    "# ID_TIME_COLS_D = {\"date\", \"cluster_id\", \"cluster_label\", \"year\", \"month\", \"day\", \"dow\"}\n",
    "# DROP_FROM_X_D = {\"departures\"}\n",
    "\n",
    "# schema_d = lf_d.collect_schema()\n",
    "# all_cols_d = schema_d.names()\n",
    "\n",
    "# ml_features_d = [\n",
    "#     c for c in all_cols_d\n",
    "#     if (c not in ID_TIME_COLS_D) and (c not in DROP_FROM_X_D)\n",
    "# ]\n",
    "\n",
    "# daily_supervised = (\n",
    "#     lf_d\n",
    "#     .sort([\"cluster_id\", \"date\"])\n",
    "#     .with_columns([\n",
    "#         pl.col(\"departures\").shift(-1).over(\"cluster_id\").alias(\"y\"),\n",
    "#         (pl.col(\"date\") + pl.duration(days=1)).alias(\"pred_date\"),\n",
    "#     ])\n",
    "#     .with_columns([\n",
    "#         add_split_daily(pl.col(\"pred_date\")),\n",
    "#     ])\n",
    "#     .filter(pl.col(\"y\").is_not_null())\n",
    "#     .select(\n",
    "#         [\"pred_date\", \"cluster_id\", \"cluster_label\", \"y\", \"split\"] + ml_features_d\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for s in [\"train\", \"val\", \"test\"]:\n",
    "#     out_path = OUT_D / f\"{s}.parquet\"\n",
    "#     daily_supervised.filter(pl.col(\"split\") == s).sink_parquet(str(out_path))\n",
    "#     print(\"Wrote\", out_path)\n",
    "\n",
    "# Quick sanity checks\n",
    "train_h = pl.scan_parquet(str(OUT_H / \"train.parquet\"))\n",
    "val_h   = pl.scan_parquet(str(OUT_H / \"val.parquet\"))\n",
    "test_h  = pl.scan_parquet(str(OUT_H / \"test.parquet\"))\n",
    "\n",
    "print(\"\\nHourly cluster rows by split:\")\n",
    "print(pl.concat([\n",
    "    train_h.select(pl.lit(\"train\").alias(\"split\"), pl.len().alias(\"n\")),\n",
    "    val_h.select(pl.lit(\"val\").alias(\"split\"), pl.len().alias(\"n\")),\n",
    "    test_h.select(pl.lit(\"test\").alias(\"split\"), pl.len().alias(\"n\")),\n",
    "]).collect())\n",
    "\n",
    "print(\"\\nHourly pred_time range:\")\n",
    "print(pl.scan_parquet(str(OUT_H / \"train.parquet\")).select(\n",
    "    pl.min(\"pred_time\").alias(\"min_pred\"),\n",
    "    pl.max(\"pred_time\").alias(\"max_pred\"),\n",
    ").collect())\n",
    "\n",
    "train_d = pl.scan_parquet(str(OUT_D / \"train.parquet\"))\n",
    "val_d   = pl.scan_parquet(str(OUT_D / \"val.parquet\"))\n",
    "test_d  = pl.scan_parquet(str(OUT_D / \"test.parquet\"))\n",
    "\n",
    "print(\"\\nDaily cluster rows by split:\")\n",
    "print(pl.concat([\n",
    "    train_d.select(pl.lit(\"train\").alias(\"split\"), pl.len().alias(\"n\")),\n",
    "    val_d.select(pl.lit(\"val\").alias(\"split\"), pl.len().alias(\"n\")),\n",
    "    test_d.select(pl.lit(\"test\").alias(\"split\"), pl.len().alias(\"n\")),\n",
    "]).collect())\n",
    "\n",
    "print(\"\\nDaily pred_date range:\")\n",
    "print(pl.scan_parquet(str(OUT_D / \"train.parquet\")).select(\n",
    "    pl.min(\"pred_date\").alias(\"min_pred\"),\n",
    "    pl.max(\"pred_date\").alias(\"max_pred\"),\n",
    ").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa7c7368-733a-4e4e-8855-0bfc2312c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "\n",
    "# XH = BASE / \"features/system/X_hourly_system.parquet\"\n",
    "# OUT_H = BASE / \"modeling/system/hourly_ml\"\n",
    "# OUT_H.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# lf = pl.scan_parquet(str(XH)).sort(\"ts_hour\")\n",
    "\n",
    "# system_hourly = (\n",
    "#     lf\n",
    "#     .with_columns([\n",
    "#         (pl.col(\"ts_hour\") + pl.duration(hours=1)).alias(\"pred_time\"),\n",
    "#         pl.col(\"trips\").shift(-1).alias(\"y\"),\n",
    "#     ])\n",
    "#     .filter(pl.col(\"y\").is_not_null())\n",
    "#     .with_columns([\n",
    "#         pl.when(pl.col(\"pred_time\") <= pl.datetime(2021, 12, 31, 23, 0, 0)).then(pl.lit(\"train\"))\n",
    "#           .when(pl.col(\"pred_time\") <= pl.datetime(2022, 12, 31, 23, 0, 0)).then(pl.lit(\"val\"))\n",
    "#           .otherwise(pl.lit(\"test\"))\n",
    "#           .alias(\"split\")\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# # Drop contemporaneous outcome to avoid leakage\n",
    "# drop_cols = [\"trips\", \"ts_hour\"]\n",
    "\n",
    "# for s in [\"train\", \"val\", \"test\"]:\n",
    "#     out_path = OUT_H / f\"{s}.parquet\"\n",
    "#     (\n",
    "#         system_hourly\n",
    "#         .filter(pl.col(\"split\") == s)\n",
    "#         .drop([\"split\"] + drop_cols)\n",
    "#         .sink_parquet(str(out_path))\n",
    "#     )\n",
    "#     print(\"Wrote\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a3ec21-2fe7-4581-ab4d-b54a3c3858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XD = BASE / \"features/system/X_daily_system.parquet\"\n",
    "# OUT_D = BASE / \"modeling/system/daily_ml\"\n",
    "# OUT_D.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# lfd = pl.scan_parquet(str(XD)).sort(\"date\")\n",
    "\n",
    "# system_daily = (\n",
    "#     lfd\n",
    "#     .with_columns([\n",
    "#         (pl.col(\"date\") + pl.duration(days=1)).alias(\"pred_date\"),\n",
    "#         pl.col(\"trips\").shift(-1).alias(\"y\"),\n",
    "#     ])\n",
    "#     .filter(pl.col(\"y\").is_not_null())\n",
    "#     .with_columns([\n",
    "#         pl.when(pl.col(\"pred_date\") <= pl.datetime(2021, 12, 31)).then(pl.lit(\"train\"))\n",
    "#           .when(pl.col(\"pred_date\") <= pl.datetime(2022, 12, 31)).then(pl.lit(\"val\"))\n",
    "#           .otherwise(pl.lit(\"test\"))\n",
    "#           .alias(\"split\")\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# drop_cols = [\"trips\", \"date\"]\n",
    "\n",
    "# for s in [\"train\", \"val\", \"test\"]:\n",
    "#     out_path = OUT_D / f\"{s}.parquet\"\n",
    "#     (\n",
    "#         system_daily\n",
    "#         .filter(pl.col(\"split\") == s)\n",
    "#         .drop([\"split\"] + drop_cols)\n",
    "#         .sink_parquet(str(out_path))\n",
    "#     )\n",
    "#     print(\"Wrote\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e37224b-de35-4837-98ce-769063839c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "\n",
    "# DATASETS = [\n",
    "#     BASE / \"modeling/system/hourly_ml\",\n",
    "#     BASE / \"modeling/system/daily_ml\",\n",
    "#     BASE / \"modeling/cluster/hourly_ml\",\n",
    "#     BASE / \"modeling/cluster/daily_ml\",\n",
    "#     BASE / \"modeling/station/hourly_ml\",\n",
    "# ]\n",
    "\n",
    "# HOLIDAY_COLS = [\"holiday_short\", \"holiday_name\", \"holiday_id\"]\n",
    "\n",
    "# for ds_dir in DATASETS:\n",
    "#     for split in [\"train\", \"val\", \"test\"]:\n",
    "#         path = ds_dir / f\"{split}.parquet\"\n",
    "#         lf = pl.scan_parquet(str(path))\n",
    "#         schema = lf.collect_schema()\n",
    "#         cols = schema.names()\n",
    "\n",
    "#         present = [c for c in HOLIDAY_COLS if c in cols]\n",
    "#         if not present:\n",
    "#             continue\n",
    "\n",
    "#         # is_holiday = 1 if any holiday field is non-null\n",
    "#         is_holiday = (\n",
    "#             pl.any_horizontal([pl.col(c).is_not_null() for c in present])\n",
    "#             .cast(pl.Int8)\n",
    "#             .alias(\"is_holiday\")\n",
    "#         )\n",
    "\n",
    "#         out = (\n",
    "#             lf.with_columns([is_holiday])\n",
    "#               .drop(present)\n",
    "#         )\n",
    "\n",
    "#         tmp = ds_dir / f\"_{split}.tmp.parquet\"\n",
    "#         out.sink_parquet(str(tmp))\n",
    "#         tmp.replace(path)\n",
    "\n",
    "#     print(\"Updated:\", ds_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0316fe5-fef6-4ec5-9c43-c5e8ca243c30",
   "metadata": {},
   "source": [
    "## Random Forest and XGBoost\n",
    "\n",
    "At the system, station, and cluster levels, tree-based models were implemented as follows:\n",
    "\n",
    "* Tabular feature matrices were constructed with lagged demand variables:\n",
    "\n",
    "  * Lagged target values (e.g., `y_{t−1}`, `y_{t−2}`, `y_{t−24}`) were included as features to capture autoregressive effects.\n",
    "\n",
    "* Hyperparameter tuning was conducted using validation data:\n",
    "\n",
    "  * For Random Forest models, the number of trees, maximum tree depth, and minimum samples per split and leaf were tuned.\n",
    "  * For XGBoost models, the learning rate, maximum depth, subsampling rate, and number of estimators were tuned.\n",
    "\n",
    "Validation sets were used to select hyperparameters, after which models were retrained on the combined training and validation data for final evaluation on the test set.\n",
    "\n",
    "Model performance was assessed using RMSE, MAE, and MAPE for one-step-ahead forecasts, with multi-step forecasts evaluated where applicable using recursive prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004abb8-99aa-440e-b2a3-c1287cdc0047",
   "metadata": {},
   "source": [
    "### Random Forest baseline (system-level hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "441a6215-a51c-4bd6-84ce-d850345fe1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43823, 54) (8760, 54) (17544, 54)\n"
     ]
    }
   ],
   "source": [
    "# Load and split data\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "SYS_H = BASE / \"modeling/system/hourly_ml\"\n",
    "\n",
    "def load_split(split):\n",
    "    return pd.read_parquet(SYS_H / f\"{split}.parquet\")\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df   = load_split(\"val\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "TARGET = \"y\"\n",
    "DROP_COLS = [\"pred_time\"]\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a23f5a55-45b7-404e-acc9-8db6fc032e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping datetime columns: ['date']\n",
      "Dropping datetime columns: ['date']\n",
      "Dropping datetime columns: ['date']\n",
      "Final X shapes: (43823, 53) (8760, 53) (17544, 53)\n",
      "Categoricals: ['day_type', 'pandemic_regime']\n",
      "Num cols: 51\n"
     ]
    }
   ],
   "source": [
    "cat_cols = [\"day_type\", \"pandemic_regime\"]\n",
    "\n",
    "def drop_datetime_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        print(\"Dropping datetime columns:\", dt_cols)\n",
    "        return X.drop(columns=dt_cols)\n",
    "    return X\n",
    "\n",
    "X_train = drop_datetime_cols(X_train)\n",
    "X_val   = drop_datetime_cols(X_val)\n",
    "X_test  = drop_datetime_cols(X_test)\n",
    "\n",
    "obj_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\" and c not in cat_cols]\n",
    "if obj_cols:\n",
    "    print(\"Dropping unexpected object columns:\", obj_cols)\n",
    "    X_train = X_train.drop(columns=obj_cols)\n",
    "    X_val   = X_val.drop(columns=obj_cols)\n",
    "    X_test  = X_test.drop(columns=obj_cols)\n",
    "\n",
    "# Recompute numeric columns after dropping\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Categoricals:\", cat_cols)\n",
    "print(\"Num cols:\", len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2440b3c-a870-4589-b4bf-7bceeec0581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categoricals\n",
    "\n",
    "cat_cols = [\"day_type\", \"pandemic_regime\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e471c589-fd27-432b-81d2-7c016b1d79b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__n_estimators': 300, 'rf__max_depth': 15, 'rf__min_samples_leaf': 5} → val RMSE: 633.73\n",
      "{'rf__n_estimators': 300, 'rf__max_depth': 25, 'rf__min_samples_leaf': 5} → val RMSE: 630.86\n",
      "{'rf__n_estimators': 600, 'rf__max_depth': 15, 'rf__min_samples_leaf': 5} → val RMSE: 634.16\n",
      "{'rf__n_estimators': 600, 'rf__max_depth': 25, 'rf__min_samples_leaf': 5} → val RMSE: 631.26\n",
      "{'rf__n_estimators': 600, 'rf__max_depth': None, 'rf__min_samples_leaf': 10} → val RMSE: 651.49\n",
      "\n",
      "Best params: {'rf__n_estimators': 300, 'rf__max_depth': 25, 'rf__min_samples_leaf': 5}\n",
      "Best val RMSE: 630.86\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "param_grid = [\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 15, \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 25, \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 600, \"rf__max_depth\": 15, \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 600, \"rf__max_depth\": 25, \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 600, \"rf__max_depth\": None, \"rf__min_samples_leaf\": 10},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1)),\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params)\n",
    "print(\"Best val RMSE:\", round(best_rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dfa1b19-e7fb-453b-b323-4b77d4f01702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (system hourly RF)\n",
      "RMSE: 1181.33\n",
      "MAE : 688.08\n",
      "MAPE: 0.2346\n",
      "R²  : 0.8997\n"
     ]
    }
   ],
   "source": [
    "# Retrain on train + val, evaluate on test\n",
    "\n",
    "# Combine train + val\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# Build pipeline\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply tuned params (these include rf__ prefixes)\n",
    "final_model.set_params(**best_params)\n",
    "\n",
    "# Fit and predict\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Metrics (RMSE without squared=)\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "# Safe MAPE (handles zeros)\n",
    "eps = 1.0\n",
    "mape = np.mean(np.abs(y_test.to_numpy() - test_pred) / np.maximum(np.abs(y_test.to_numpy()), eps))\n",
    "\n",
    "print(\"\\nTEST METRICS (system hourly RF)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ab0b4-3e2a-40f8-af2e-c6d45904be14",
   "metadata": {},
   "source": [
    "### XGBoost baseline (system-level hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb4761d2-085f-427d-9812-bcc985daf0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping datetime columns: ['date']\n",
      "Dropping datetime columns: ['date']\n",
      "Dropping datetime columns: ['date']\n",
      "Final X shapes: (43823, 53) (8760, 53) (17544, 53)\n",
      "Num cols: 51 | Cat cols: ['day_type', 'pandemic_regime']\n"
     ]
    }
   ],
   "source": [
    "# Load data and build X/y\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "SYS_H = BASE / \"modeling/system/hourly_ml\"\n",
    "\n",
    "def load_split(split):\n",
    "    return pd.read_parquet(SYS_H / f\"{split}.parquet\")\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df   = load_split(\"val\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "TARGET = \"y\"\n",
    "DROP_COLS = [\"pred_time\"]  # not a feature\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "# Drop datetime columns if present (version-proof)\n",
    "def drop_datetime_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        print(\"Dropping datetime columns:\", dt_cols)\n",
    "        return X.drop(columns=dt_cols)\n",
    "    return X\n",
    "\n",
    "X_train = drop_datetime_cols(X_train)\n",
    "X_val   = drop_datetime_cols(X_val)\n",
    "X_test  = drop_datetime_cols(X_test)\n",
    "\n",
    "# Known categoricals\n",
    "cat_cols = [\"day_type\", \"pandemic_regime\"]\n",
    "\n",
    "# Safety: drop any unexpected object columns besides categoricals\n",
    "obj_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\" and c not in cat_cols]\n",
    "if obj_cols:\n",
    "    print(\"Dropping unexpected object columns:\", obj_cols)\n",
    "    X_train = X_train.drop(columns=obj_cols)\n",
    "    X_val   = X_val.drop(columns=obj_cols)\n",
    "    X_test  = X_test.drop(columns=obj_cols)\n",
    "\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Num cols:\", len(num_cols), \"| Cat cols:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f934e43-038e-4c1b-8fa5-48654939f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess one-hot categoricals\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79f59aa0-4d9e-4d1f-a5a2-159c5d90bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__n_estimators': 500, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 6, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 632.97\n",
      "{'xgb__n_estimators': 800, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 6, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 636.26\n",
      "{'xgb__n_estimators': 800, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 8, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 618.69\n",
      "{'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 6, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 632.3\n",
      "{'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 8, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 619.06\n",
      "\n",
      "Best params: {'xgb__n_estimators': 800, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 8, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8}\n",
      "Best val RMSE: 618.69\n"
     ]
    }
   ],
   "source": [
    "# Tune XGBoost on validation RMSE\n",
    "\n",
    "param_grid = [\n",
    "    {\"xgb__n_estimators\": 500,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 6,  \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 800,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 6,  \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 800,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 8,  \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 1200, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 6,  \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 1200, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 8,  \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "]\n",
    "\n",
    "# ---- fail fast if the wrong grid is being used (prevents \"nonsense tuning\") ----\n",
    "bad = [p for p in param_grid if not any(k.startswith(\"xgb__\") for k in p)]\n",
    "if bad:\n",
    "    raise ValueError(\n",
    "        \"param_grid contains non-XGBoost parameter dict(s). \"\n",
    "        f\"Example keys: {list(bad[0].keys())}. \"\n",
    "        \"Expected keys like 'xgb__n_estimators', 'xgb__max_depth', etc.\"\n",
    "    )\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"xgb\", XGBRegressor(\n",
    "                random_state=42,\n",
    "                n_jobs=1,\n",
    "                objective=\"reg:squarederror\",\n",
    "                tree_method=\"hist\",\n",
    "                reg_lambda=1.0,\n",
    "                min_child_weight=1,\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "    # cleanup helps avoid peak-memory creep in notebooks\n",
    "    del model, val_pred\n",
    "    gc.collect()\n",
    "\n",
    "best_params, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params)\n",
    "print(\"Best val RMSE:\", round(best_rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a28156cc-1436-4ae7-8a2c-d51125d3d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (system hourly XGBoost)\n",
      "RMSE: 1398.48\n",
      "MAE : 815.34\n",
      "MAPE: 0.2512\n",
      "R²  : 0.8594\n"
     ]
    }
   ],
   "source": [
    "# Retrain on train+val and evaluate on test\n",
    "\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\", XGBRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            reg_lambda=1.0,\n",
    "            min_child_weight=1,\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "final_model.set_params(**best_params)\n",
    "\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "# Safe MAPE (handles zeros)\n",
    "eps = 1.0\n",
    "mape = np.mean(np.abs(y_test.to_numpy() - test_pred) / np.maximum(np.abs(y_test.to_numpy()), eps))\n",
    "\n",
    "print(\"\\nTEST METRICS (system hourly XGBoost)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a03ac-fda8-4faf-8660-868f02032e0c",
   "metadata": {},
   "source": [
    "### Random Forest (system-level daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1e2c5b1-0b16-4de7-a5f0-2bfbc4937bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system-daily data and build X / y\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "SYS_D = BASE / \"modeling/system/daily_ml\"\n",
    "\n",
    "def load_split(split):\n",
    "    return pd.read_parquet(SYS_D / f\"{split}.parquet\")\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df   = load_split(\"val\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "TARGET = \"y\"\n",
    "DROP_COLS = [\"pred_date\"]\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_test = test_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dfc5a8d-7715-45a1-9488-e0398de030a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shapes: (1825, 47) (365, 47) (731, 47)\n",
      "Num cols: 45 | Cat cols: ['day_type', 'pandemic_regime']\n"
     ]
    }
   ],
   "source": [
    "# Drop datetime columns + identify categoricals\n",
    "\n",
    "def drop_datetime_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        print(\"Dropping datetime columns:\", dt_cols)\n",
    "        return X.drop(columns=dt_cols)\n",
    "    return X\n",
    "\n",
    "X_train = drop_datetime_cols(X_train)\n",
    "X_val   = drop_datetime_cols(X_val)\n",
    "X_test  = drop_datetime_cols(X_test)\n",
    "\n",
    "cat_cols = [\"day_type\", \"pandemic_regime\"]\n",
    "\n",
    "# Drop unexpected object columns\n",
    "obj_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\" and c not in cat_cols]\n",
    "if obj_cols:\n",
    "    print(\"Dropping unexpected object columns:\", obj_cols)\n",
    "    X_train = X_train.drop(columns=obj_cols)\n",
    "    X_val   = X_val.drop(columns=obj_cols)\n",
    "    X_test  = X_test.drop(columns=obj_cols)\n",
    "\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Num cols:\", len(num_cols), \"| Cat cols:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e9eb3b3-2150-4573-9449-f05fba2da388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e82ab4e2-6a74-44c3-8ac6-d4f2c068b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_leaf': 3} → val RMSE: 17933.28\n",
      "{'rf__n_estimators': 300, 'rf__max_depth': 15, 'rf__min_samples_leaf': 3} → val RMSE: 17744.04\n",
      "{'rf__n_estimators': 300, 'rf__max_depth': 20, 'rf__min_samples_leaf': 5} → val RMSE: 17584.48\n",
      "{'rf__n_estimators': 500, 'rf__max_depth': 20, 'rf__min_samples_leaf': 5} → val RMSE: 17609.86\n",
      "{'rf__n_estimators': 500, 'rf__max_depth': None, 'rf__min_samples_leaf': 10} → val RMSE: 17379.54\n",
      "\n",
      "Best params: {'rf__n_estimators': 500, 'rf__max_depth': None, 'rf__min_samples_leaf': 10}\n",
      "Best val RMSE: 17379.54\n"
     ]
    }
   ],
   "source": [
    "# RF validation-based hyperparameter tuning\n",
    "\n",
    "param_grid = [\n",
    "    {\"rf__n_estimators\": 200, \"rf__max_depth\": 10,  \"rf__min_samples_leaf\": 3},\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 15,  \"rf__min_samples_leaf\": 3},\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 20,  \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 500, \"rf__max_depth\": 20,  \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 500, \"rf__max_depth\": None,\"rf__min_samples_leaf\": 10},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params)\n",
    "print(\"Best val RMSE:\", round(best_rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d5bd595-9837-418d-b2c8-a8d4cb060efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (system daily RF)\n",
      "RMSE: 31043.79\n",
      "MAE : 24631.25\n",
      "MAPE: 0.2556\n",
      "R²  : 0.4215\n"
     ]
    }
   ],
   "source": [
    "# Retrain on train+val and evaluate on test\n",
    "\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"rf\", RandomForestRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "final_model.set_params(**best_params)\n",
    "\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "# Safe MAPE\n",
    "eps = 1.0\n",
    "mape = np.mean(np.abs(y_test.to_numpy() - test_pred) /\n",
    "               np.maximum(np.abs(y_test.to_numpy()), eps))\n",
    "\n",
    "print(\"\\nTEST METRICS (system daily RF)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad43dd-91d7-4204-92a8-ac08f6904144",
   "metadata": {},
   "source": [
    "### XGBoost (system-level daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4211a3a-331e-4fc4-8ab1-2e4ba41a9cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shapes: (1825, 47) (365, 47) (731, 47)\n",
      "Num cols: 45 | Cat cols: ['day_type', 'pandemic_regime']\n"
     ]
    }
   ],
   "source": [
    "# Load + build X/y (daily)\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "SYS_D = BASE / \"modeling/system/daily_ml\"\n",
    "\n",
    "def load_split(split):\n",
    "    return pd.read_parquet(SYS_D / f\"{split}.parquet\")\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df   = load_split(\"val\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "TARGET = \"y\"\n",
    "DROP_COLS = [\"pred_date\"]\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "# Drop datetime columns if any\n",
    "def drop_datetime_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        print(\"Dropping datetime columns:\", dt_cols)\n",
    "        return X.drop(columns=dt_cols)\n",
    "    return X\n",
    "\n",
    "X_train = drop_datetime_cols(X_train)\n",
    "X_val   = drop_datetime_cols(X_val)\n",
    "X_test  = drop_datetime_cols(X_test)\n",
    "\n",
    "cat_cols = [\"day_type\", \"pandemic_regime\"]\n",
    "\n",
    "# Drop unexpected object columns besides categoricals\n",
    "obj_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\" and c not in cat_cols]\n",
    "if obj_cols:\n",
    "    print(\"Dropping unexpected object columns:\", obj_cols)\n",
    "    X_train = X_train.drop(columns=obj_cols)\n",
    "    X_val   = X_val.drop(columns=obj_cols)\n",
    "    X_test  = X_test.drop(columns=obj_cols)\n",
    "\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Num cols:\", len(num_cols), \"| Cat cols:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff0eb505-0caf-4bd1-888b-e0fde46afd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess one-hot categoricals\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1630ee8d-94f1-44ed-a18d-1d40da820593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__n_estimators': 600, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 17076.47\n",
      "{'xgb__n_estimators': 900, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 17198.81\n",
      "{'xgb__n_estimators': 900, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 4, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 17544.31\n",
      "{'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 3, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 16999.79\n",
      "{'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 4, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8} → val RMSE: 16913.64\n",
      "\n",
      "Best params: {'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 4, 'xgb__subsample': 0.8, 'xgb__colsample_bytree': 0.8}\n",
      "Best val RMSE: 16913.64\n"
     ]
    }
   ],
   "source": [
    "# Tune XGBoost on validation RMSE daily\n",
    "\n",
    "param_grid = [\n",
    "    {\"xgb__n_estimators\": 600,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 3, \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 900,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 3, \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 900,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 4, \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 1200, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 3, \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "    {\"xgb__n_estimators\": 1200, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 4, \"xgb__subsample\": 0.8, \"xgb__colsample_bytree\": 0.8},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"xgb\", XGBRegressor(\n",
    "                random_state=42,\n",
    "                n_jobs=1,\n",
    "                objective=\"reg:squarederror\",\n",
    "                tree_method=\"hist\",\n",
    "                reg_lambda=1.0,\n",
    "                min_child_weight=1,\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params)\n",
    "print(\"Best val RMSE:\", round(best_rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5d1f2a2-c2bd-4cce-8917-7c4b3c72127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (system daily XGBoost)\n",
      "RMSE: 36617.58\n",
      "MAE : 29242.08\n",
      "MAPE: 0.2781\n",
      "R²  : 0.1951\n"
     ]
    }
   ],
   "source": [
    "# Retrain on train+val and evaluate on test\n",
    "\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\", XGBRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            reg_lambda=1.0,\n",
    "            min_child_weight=1,\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "final_model.set_params(**best_params)\n",
    "\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "eps = 1.0\n",
    "mape = np.mean(np.abs(y_test.to_numpy() - test_pred) /\n",
    "               np.maximum(np.abs(y_test.to_numpy()), eps))\n",
    "\n",
    "print(\"\\nTEST METRICS (system daily XGBoost)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35660b93-7279-4479-a89d-9ebabb8f4a29",
   "metadata": {},
   "source": [
    "### Random Forest (cluster-level hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94d41871-da0a-4ead-8f5b-c0b6571ec904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shapes: (108369, 105) (26277, 105) (52626, 105)\n"
     ]
    }
   ],
   "source": [
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "CL_H = BASE / \"modeling/cluster/hourly_ml\"\n",
    "\n",
    "def load_split(split: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(CL_H / f\"{split}.parquet\")\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df   = load_split(\"val\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "TARGET = \"y\"\n",
    "DROP_COLS = [\"pred_time\"]\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(\"Initial X shapes:\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8a5785f-8e52-4a9b-8fb3-267ce9d14a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shapes: (108369, 104) (26277, 104) (52626, 104)\n",
      "Categoricals (final): ['cluster_label', 'pandemic_regime']\n",
      "Num cols: 102\n"
     ]
    }
   ],
   "source": [
    "def drop_datetime_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        print(\"Dropping datetime columns:\", dt_cols)\n",
    "        return X.drop(columns=dt_cols)\n",
    "    return X\n",
    "\n",
    "X_train = drop_datetime_cols(X_train)\n",
    "X_val   = drop_datetime_cols(X_val)\n",
    "X_test  = drop_datetime_cols(X_test)\n",
    "\n",
    "# Drop leakage column if present\n",
    "for df in (X_train, X_val, X_test):\n",
    "    if \"split\" in df.columns:\n",
    "        df.drop(columns=[\"split\"], inplace=True)\n",
    "\n",
    "# Auto-detect categoricals (object/string), keep only low-cardinality ones\n",
    "cat_cols = list(X_train.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "LOW_CARD_MAX = 50\n",
    "kept_cat_cols = []\n",
    "dropped_high_card = []\n",
    "for c in cat_cols:\n",
    "    n_unique = X_train[c].nunique(dropna=True)\n",
    "    if n_unique <= LOW_CARD_MAX:\n",
    "        kept_cat_cols.append(c)\n",
    "    else:\n",
    "        dropped_high_card.append((c, n_unique))\n",
    "\n",
    "if dropped_high_card:\n",
    "    print(\"Dropping high-cardinality categoricals:\", dropped_high_card)\n",
    "\n",
    "cat_cols = kept_cat_cols\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Categoricals (final):\", cat_cols)\n",
    "print(\"Num cols:\", len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "451b4e69-0d42-4c58-b81b-d8368ec02b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot categoricals\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cd0bce0-74aa-4885-bd2c-6a1bece605c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__n_estimators': 300, 'rf__max_depth': 15, 'rf__min_samples_leaf': 3} → val RMSE: 279.16\n",
      "{'rf__n_estimators': 300, 'rf__max_depth': 25, 'rf__min_samples_leaf': 3} → val RMSE: 276.37\n",
      "{'rf__n_estimators': 600, 'rf__max_depth': 15, 'rf__min_samples_leaf': 3} → val RMSE: 278.98\n",
      "{'rf__n_estimators': 600, 'rf__max_depth': 25, 'rf__min_samples_leaf': 3} → val RMSE: 276.11\n",
      "{'rf__n_estimators': 600, 'rf__max_depth': None, 'rf__min_samples_leaf': 5} → val RMSE: 279.67\n",
      "\n",
      "Best params: {'rf__n_estimators': 600, 'rf__max_depth': 25, 'rf__min_samples_leaf': 3}\n",
      "Best val RMSE: 276.11\n"
     ]
    }
   ],
   "source": [
    "# RF tuning on validation RMSE\n",
    "\n",
    "param_grid = [\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 15,  \"rf__min_samples_leaf\": 3},\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 25,  \"rf__min_samples_leaf\": 3},\n",
    "    {\"rf__n_estimators\": 600, \"rf__max_depth\": 15,  \"rf__min_samples_leaf\": 3},\n",
    "    {\"rf__n_estimators\": 600, \"rf__max_depth\": 25,  \"rf__min_samples_leaf\": 3},\n",
    "    {\"rf__n_estimators\": 600, \"rf__max_depth\": None,\"rf__min_samples_leaf\": 5},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1)),\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params)\n",
    "print(\"Best val RMSE:\", round(best_rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7628ff0c-a9d0-4863-a9ac-4465a1cd37a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (cluster hourly RF — corrected, no leakage)\n",
      "RMSE: 409.47\n",
      "MAE : 205.35\n",
      "MAPE: 0.1978\n",
      "R²  : 0.9516\n"
     ]
    }
   ],
   "source": [
    "# Retrain on train+val and evaluate on test\n",
    "\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "final_model.set_params(**best_params)\n",
    "\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "eps = 1.0\n",
    "mape = np.mean(np.abs(y_test.to_numpy() - test_pred) /\n",
    "               np.maximum(np.abs(y_test.to_numpy()), eps))\n",
    "\n",
    "print(\"\\nTEST METRICS (cluster hourly RF — corrected, no leakage)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "791c02ac-0989-4fdf-b706-0f218a8992ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 'split' column is not present in any X split\n"
     ]
    }
   ],
   "source": [
    "assert \"split\" not in X_train.columns and \"split\" not in X_val.columns and \"split\" not in X_test.columns\n",
    "print(\"✔ 'split' column is not present in any X split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fdc175-1329-422c-864b-cf5b5a55ba97",
   "metadata": {},
   "source": [
    "### XGBoost (cluster-level hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e488948-81f4-4dce-9fdf-7072feaac3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__n_estimators': 600, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 5} → val RMSE: 280.87\n",
      "{'xgb__n_estimators': 800, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 5} → val RMSE: 279.0\n",
      "{'xgb__n_estimators': 800, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 7} → val RMSE: 273.41\n",
      "{'xgb__n_estimators': 1000, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 5} → val RMSE: 278.46\n",
      "{'xgb__n_estimators': 1000, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 7} → val RMSE: 274.03\n",
      "\n",
      "Best params: {'xgb__n_estimators': 800, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 7}\n",
      "Best val RMSE: 273.41\n"
     ]
    }
   ],
   "source": [
    "# XGBoost tuning\n",
    "\n",
    "param_grid = [\n",
    "    {\"xgb__n_estimators\": 600,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 5},\n",
    "    {\"xgb__n_estimators\": 800,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 5},\n",
    "    {\"xgb__n_estimators\": 800,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 7},\n",
    "    {\"xgb__n_estimators\": 1000, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 5},\n",
    "    {\"xgb__n_estimators\": 1000, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 7},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"xgb\", XGBRegressor(\n",
    "                random_state=42,\n",
    "                n_jobs=1,\n",
    "                objective=\"reg:squarederror\",\n",
    "                tree_method=\"hist\",\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda=1.0,\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params_xgb, best_rmse_xgb = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params_xgb)\n",
    "print(\"Best val RMSE:\", round(best_rmse_xgb, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f9d661e-8756-469f-aa1d-4bcf3d4cd5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (cluster hourly XGBoost)\n",
      "RMSE: 495.41\n",
      "MAE : 237.72\n",
      "MAPE: 0.2012\n",
      "R²  : 0.9291\n"
     ]
    }
   ],
   "source": [
    "# Retrain on train+val and evaluate on test\n",
    "\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model_xgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\", XGBRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "final_model_xgb.set_params(**best_params_xgb)\n",
    "\n",
    "final_model_xgb.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model_xgb.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "eps = 1.0\n",
    "mape = np.mean(np.abs(y_test.to_numpy() - test_pred) /\n",
    "               np.maximum(np.abs(y_test.to_numpy()), eps))\n",
    "\n",
    "print(\"\\nTEST METRICS (cluster hourly XGBoost)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab600062-89f1-4190-8961-ea92d6b0b576",
   "metadata": {},
   "source": [
    "### Random Forest (cluster-level daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7106905e-bcc2-483a-84f3-3674ea5c266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cluster DAILY shapes: (4545, 103) (1095, 103) (2193, 103)\n",
      "Initial X shapes: (4545, 101) (1095, 101) (2193, 101)\n"
     ]
    }
   ],
   "source": [
    "# Load cluster daily data and build X/y\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "CL_D = BASE / \"modeling/cluster/daily_ml\"\n",
    "\n",
    "def load_split(split: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(CL_D / f\"{split}.parquet\")\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df   = load_split(\"val\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "print(\"Loaded cluster DAILY shapes:\", train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "TARGET = \"y\"\n",
    "DROP_COLS = [\"pred_date\"]  # daily key\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET] + DROP_COLS)\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(\"Initial X shapes:\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8924cbcf-ca5c-43f4-a04f-1d0a3451b0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shapes: (4545, 100) (1095, 100) (2193, 100)\n",
      "Categoricals (final): ['cluster_label', 'pandemic_regime']\n",
      "Num cols: 98\n"
     ]
    }
   ],
   "source": [
    "# Drop datetime columns + remove leakage + set cat/num cols\n",
    "\n",
    "def drop_datetime_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        print(\"Dropping datetime columns:\", dt_cols)\n",
    "        return X.drop(columns=dt_cols)\n",
    "    return X\n",
    "\n",
    "X_train = drop_datetime_cols(X_train)\n",
    "X_val   = drop_datetime_cols(X_val)\n",
    "X_test  = drop_datetime_cols(X_test)\n",
    "\n",
    "# Drop leakage column if present (this caused your error: 'train'/'val'/'test' strings)\n",
    "for df in (X_train, X_val, X_test):\n",
    "    if \"split\" in df.columns:\n",
    "        df.drop(columns=[\"split\"], inplace=True)\n",
    "\n",
    "# Cluster daily categoricals should match hourly cluster setup (minus split)\n",
    "cat_cols = [c for c in X_train.select_dtypes(include=[\"object\"]).columns]\n",
    "\n",
    "LOW_CARD_MAX = 50\n",
    "cat_cols = [c for c in cat_cols if X_train[c].nunique(dropna=True) <= LOW_CARD_MAX]\n",
    "\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Categoricals (final):\", cat_cols)\n",
    "print(\"Num cols:\", len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c64f0fd-6410-48bd-b0b0-1e251c4c28cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__n_estimators': 300, 'rf__max_depth': 10, 'rf__min_samples_leaf': 5} → val RMSE: 7497.6\n",
      "{'rf__n_estimators': 500, 'rf__max_depth': 15, 'rf__min_samples_leaf': 5} → val RMSE: 7487.59\n",
      "{'rf__n_estimators': 500, 'rf__max_depth': 20, 'rf__min_samples_leaf': 10} → val RMSE: 7464.15\n",
      "{'rf__n_estimators': 800, 'rf__max_depth': None, 'rf__min_samples_leaf': 10} → val RMSE: 7449.23\n",
      "\n",
      "Best params: {'rf__n_estimators': 800, 'rf__max_depth': None, 'rf__min_samples_leaf': 10}\n",
      "Best val RMSE: 7449.23\n"
     ]
    }
   ],
   "source": [
    "# Preprocess + RF tuning (validation RMSE)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\"rf__n_estimators\": 300, \"rf__max_depth\": 10,  \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 500, \"rf__max_depth\": 15,  \"rf__min_samples_leaf\": 5},\n",
    "    {\"rf__n_estimators\": 500, \"rf__max_depth\": 20,  \"rf__min_samples_leaf\": 10},\n",
    "    {\"rf__n_estimators\": 800, \"rf__max_depth\": None,\"rf__min_samples_leaf\": 10},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1)),\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params_rf, best_rmse_rf = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params_rf)\n",
    "print(\"Best val RMSE:\", round(best_rmse_rf, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2361e051-fcfa-4c7d-abaf-4f1697adfb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (cluster daily RF)\n",
      "RMSE: 11197.57\n",
      "MAE : 6870.58\n",
      "MAPE: 0.2273\n",
      "R²  : 0.8615\n"
     ]
    }
   ],
   "source": [
    "# Final test evaluation\n",
    "\n",
    "# Combine train and validation\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"rf\", RandomForestRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "final_model.set_params(**best_params_rf)\n",
    "\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Test prediction\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "# Safe MAPE\n",
    "eps = 1.0\n",
    "mape = np.mean(\n",
    "    np.abs(y_test.to_numpy() - test_pred) /\n",
    "    np.maximum(np.abs(y_test.to_numpy()), eps)\n",
    ")\n",
    "\n",
    "print(\"\\nTEST METRICS (cluster daily RF)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db2ddd-c3d6-4b93-bc0f-7a321279ddf9",
   "metadata": {},
   "source": [
    "### XGBoost (cluster-level daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d80febd-80d5-4ed6-bc53-5250e61a3b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__n_estimators': 600, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3} → val RMSE: 7282.3\n",
      "{'xgb__n_estimators': 900, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3} → val RMSE: 7318.45\n",
      "{'xgb__n_estimators': 900, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 4} → val RMSE: 7245.45\n",
      "{'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 3} → val RMSE: 7279.9\n",
      "{'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 4} → val RMSE: 7216.31\n",
      "\n",
      "Best params: {'xgb__n_estimators': 1200, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 4}\n",
      "Best val RMSE: 7216.31\n"
     ]
    }
   ],
   "source": [
    "# XGBoost validation tuning\n",
    "\n",
    "param_grid = [\n",
    "    {\"xgb__n_estimators\": 600,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 3},\n",
    "    {\"xgb__n_estimators\": 900,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 3},\n",
    "    {\"xgb__n_estimators\": 900,  \"xgb__learning_rate\": 0.05, \"xgb__max_depth\": 4},\n",
    "    {\"xgb__n_estimators\": 1200, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 3},\n",
    "    {\"xgb__n_estimators\": 1200, \"xgb__learning_rate\": 0.03, \"xgb__max_depth\": 4},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", preprocess),\n",
    "            (\"xgb\", XGBRegressor(\n",
    "                random_state=42,\n",
    "                n_jobs=1,\n",
    "                objective=\"reg:squarederror\",\n",
    "                tree_method=\"hist\",\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda=1.0,\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "    model.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
    "    results.append((params, rmse))\n",
    "    print(params, \"→ val RMSE:\", round(rmse, 2))\n",
    "\n",
    "best_params_xgb, best_rmse_xgb = min(results, key=lambda x: x[1])\n",
    "print(\"\\nBest params:\", best_params_xgb)\n",
    "print(\"Best val RMSE:\", round(best_rmse_xgb, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7eccc4d-692a-4f13-a435-1abd1c5fd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS (cluster daily XGBoost)\n",
      "RMSE: 13726.23\n",
      "MAE : 7867.53\n",
      "MAPE: 0.2264\n",
      "R²  : 0.7920\n"
     ]
    }
   ],
   "source": [
    "# Final train+val -> test evaluation\n",
    "\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_model_xgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\", XGBRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "final_model_xgb.set_params(**best_params_xgb)\n",
    "\n",
    "final_model_xgb.fit(X_trainval, y_trainval)\n",
    "test_pred = final_model_xgb.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, test_pred) ** 0.5\n",
    "mae  = mean_absolute_error(y_test, test_pred)\n",
    "r2   = r2_score(y_test, test_pred)\n",
    "\n",
    "eps = 1.0\n",
    "mape = np.mean(\n",
    "    np.abs(y_test.to_numpy() - test_pred) /\n",
    "    np.maximum(np.abs(y_test.to_numpy()), eps)\n",
    ")\n",
    "\n",
    "print(\"\\nTEST METRICS (cluster daily XGBoost)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb611b2-8517-4e40-a923-c8f363260fc8",
   "metadata": {},
   "source": [
    "## CNN–LSTM models\n",
    "\n",
    "For station-level short-term forecasting, convolutional–recurrent models were implemented as follows:\n",
    "\n",
    "* Input construction:\n",
    "\n",
    "  * For each station, input sequences of length `T` (e.g., the previous 24 or 48 hours) were constructed using only information available up to time *t*.\n",
    "\n",
    "    * Included past demand values (`y_{t−1}, y_{t−2}, …`).\n",
    "    * Included historical weather variables.\n",
    "    * Included calendar variables such as hour of day, day type, and holiday indicators.\n",
    "  * Future-aware rollups (e.g., centered or forward-looking rolling features) were avoided, and all rolling features were strictly backward-looking.\n",
    "  * Static station features were incorporated:\n",
    "\n",
    "    * High-cardinality identity features (e.g., `station_id`) were encoded using embeddings.\n",
    "    * Low-cardinality categorical features (e.g., cluster, borough) were encoded using embeddings or one-hot encoding.\n",
    "    * Built-environment and demographic variables were included as numeric features and concatenated with the learned sequence representation.\n",
    "\n",
    "* Architecture:\n",
    "\n",
    "  * One-dimensional convolutional layers were applied over the temporal dimension to extract short-term patterns such as intra-day dynamics.\n",
    "  * LSTM layers were stacked after the convolutional block to capture longer-term temporal dependencies.\n",
    "  * A dense output layer was used to predict `y_{t+1}`, with extensions to multi-horizon forecasting implemented where applicable using recursive or direct strategies.\n",
    "\n",
    "* Training strategy:\n",
    "\n",
    "  * A global CNN–LSTM model was trained across all stations, with a station identity embedding used to capture station-specific effects. This approach scaled efficiently and avoided training separate models for each station.\n",
    "  * Where feasible, a hybrid extension inspired by Van der Wielen et al. was explored, augmenting the global model with lightweight station-specific bias or residual corrections rather than full station-level neural networks.\n",
    "\n",
    "* Evaluation:\n",
    "\n",
    "  * Station-level RMSE, MAE, and MAPE were computed and aggregated using:\n",
    "\n",
    "    * Macro-averaging (equal weight per station) as the primary evaluation metric.\n",
    "    * Volume-weighted averaging as a secondary metric to reflect system-level demand.\n",
    "  * CNN–LSTM performance was compared against:\n",
    "\n",
    "    * Classical time-series baselines (e.g., ARIMA) where available.\n",
    "    * A fast tabular baseline trained on identical data splits (HistGradientBoosting), used in place of full station-level Random Forest or XGBoost models when computational constraints applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d17cc206-5528-43db-9962-51d0f3e50eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# Parameters and paths\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "\n",
    "IN_DIR  = BASE / \"modeling/station/hourly_ml\"\n",
    "OUT_DIR = BASE / \"modeling/station/hourly_cnn_ready\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_in = IN_DIR / \"train.parquet\"\n",
    "val_in   = IN_DIR / \"val.parquet\"\n",
    "test_in  = IN_DIR / \"test.parquet\"\n",
    "\n",
    "train_out = OUT_DIR / \"train.parquet\"\n",
    "val_out   = OUT_DIR / \"val.parquet\"\n",
    "test_out  = OUT_DIR / \"test.parquet\"\n",
    "\n",
    "TARGET = \"y\"\n",
    "ID_COL = \"station_id\"\n",
    "TIME_COL = \"pred_time\"\n",
    "\n",
    "print(train_in.exists(), val_in.exists(), test_in.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a626248a-d680-4760-b55b-fc7ab3675718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build consistent categorical mappings from TRAIN and apply to all splits\n",
    "\n",
    "MAP_PATH = OUT_DIR / \"category_maps.json\"\n",
    "\n",
    "# def get_schema_cols(path: Path):\n",
    "#     schema = pl.scan_parquet(path).collect_schema()\n",
    "#     names = schema.names()\n",
    "#     utf8_cols = [c for c, dt in schema.items() if dt == pl.Utf8]\n",
    "#     return names, utf8_cols\n",
    "\n",
    "# cols_train, utf8_train = get_schema_cols(train_in)\n",
    "\n",
    "# if TARGET not in cols_train:\n",
    "#     raise ValueError(f\"Missing target column '{TARGET}' in {train_in}\")\n",
    "# if ID_COL not in cols_train:\n",
    "#     raise ValueError(f\"Missing station id column '{ID_COL}' in {train_in}\")\n",
    "# if TIME_COL not in cols_train:\n",
    "#     raise ValueError(f\"Missing time column '{TIME_COL}' in {train_in}\")\n",
    "\n",
    "# # We'll encode all string cols except station_id (station_id handled separately -> station_idx)\n",
    "# encode_cols = [c for c in utf8_train if c != ID_COL]\n",
    "\n",
    "# # Drop leakage columns if present\n",
    "# DROP_COLS = [c for c in [\"split\"] if c in cols_train]\n",
    "\n",
    "# print(\"String cols to encode (train):\", encode_cols[:20], \"… total:\", len(encode_cols))\n",
    "# print(\"Leakage cols to drop:\", DROP_COLS)\n",
    "\n",
    "# # Build category maps from TRAIN only (consistent coding across splits)\n",
    "# maps = {}\n",
    "# for c in encode_cols:\n",
    "#     # include None explicitly as \"missing\"\n",
    "#     vals = (\n",
    "#         pl.scan_parquet(train_in)\n",
    "#         .select(pl.col(c))\n",
    "#         .unique()\n",
    "#         .collect()[c]\n",
    "#         .to_list()\n",
    "#     )\n",
    "#     # deterministic order; keep None first if present\n",
    "#     vals_sorted = sorted([v for v in vals if v is not None])\n",
    "#     if any(v is None for v in vals):\n",
    "#         vals_sorted = [None] + vals_sorted\n",
    "#     maps[c] = vals_sorted\n",
    "\n",
    "# # station_id mapping -> station_idx (embedding index)\n",
    "# station_vals = (\n",
    "#     pl.scan_parquet(train_in)\n",
    "#     .select(pl.col(ID_COL))\n",
    "#     .unique()\n",
    "#     .collect()[ID_COL]\n",
    "#     .to_list()\n",
    "# )\n",
    "# station_vals_sorted = sorted([v for v in station_vals if v is not None])\n",
    "# maps[\"__station_id__\"] = station_vals_sorted\n",
    "\n",
    "# with open(MAP_PATH, \"w\") as f:\n",
    "#     json.dump(maps, f)\n",
    "\n",
    "# print(\"Wrote category maps:\", MAP_PATH)\n",
    "# print(\"N stations (train):\", len(maps[\"__station_id__\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a1b5526-403a-4006-bed6-e02fa95e178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply mappings and write CNN-ready parquets\n",
    "\n",
    "# def apply_maps_and_write(in_path: Path, out_path: Path, maps: dict):\n",
    "#     lf = pl.scan_parquet(in_path)\n",
    "\n",
    "#     # drop leakage\n",
    "#     for c in DROP_COLS:\n",
    "#         if c in lf.collect_schema().names():\n",
    "#             lf = lf.drop(c)\n",
    "\n",
    "#     # station_idx\n",
    "#     station_map = {sid: i for i, sid in enumerate(maps[\"__station_id__\"])}\n",
    "#     lf = lf.with_columns(\n",
    "#         pl.col(ID_COL).replace(station_map, default=None).cast(pl.Int32).alias(\"station_idx\")\n",
    "#     )\n",
    "\n",
    "#     # encode other Utf8 columns\n",
    "#     schema = lf.collect_schema()\n",
    "#     utf8_cols = [c for c, dt in schema.items() if dt == pl.Utf8 and c != ID_COL]\n",
    "#     for c in utf8_cols:\n",
    "#         if c not in maps:\n",
    "#             lf = lf.drop(c)\n",
    "#             continue\n",
    "#         cmap = {v: i for i, v in enumerate(maps[c])}\n",
    "#         lf = lf.with_columns(\n",
    "#             pl.col(c).replace(cmap, default=None).cast(pl.Int16).alias(f\"{c}_code\")\n",
    "#         ).drop(c)\n",
    "\n",
    "#     # downcast floats for DL efficiency where safe\n",
    "#     # (leave TIME_COL as datetime; we need it to form sequences)\n",
    "#     out_schema = lf.collect_schema()\n",
    "#     float_cols = [c for c, dt in out_schema.items() if dt in (pl.Float64,)]\n",
    "#     if float_cols:\n",
    "#         lf = lf.with_columns([pl.col(c).cast(pl.Float32) for c in float_cols])\n",
    "\n",
    "#     # ensure target is float32\n",
    "#     lf = lf.with_columns(pl.col(TARGET).cast(pl.Float32))\n",
    "\n",
    "#     # write\n",
    "#     lf.sink_parquet(out_path)\n",
    "#     print(\"Wrote\", out_path)\n",
    "\n",
    "# # load maps from file to avoid accidental drift\n",
    "# with open(MAP_PATH, \"r\") as f:\n",
    "#     maps_loaded = json.load(f)\n",
    "\n",
    "# apply_maps_and_write(train_in, train_out, maps_loaded)\n",
    "# apply_maps_and_write(val_in,   val_out,   maps_loaded)\n",
    "# apply_maps_and_write(test_in,  test_out,  maps_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b2e9e56-57e1-4cc2-abb1-a14181dafdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.parquet rows: 48151826 | utf8 remaining: ['station_id']\n",
      "shape: (1, 2)\n",
      "┌─────────────────────┬─────────────────────┐\n",
      "│ min                 ┆ max                 │\n",
      "│ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ datetime[μs]        │\n",
      "╞═════════════════════╪═════════════════════╡\n",
      "│ 2017-01-01 01:00:00 ┆ 2021-12-31 23:00:00 │\n",
      "└─────────────────────┴─────────────────────┘\n",
      "val.parquet rows: 16046488 | utf8 remaining: ['station_id']\n",
      "shape: (1, 2)\n",
      "┌─────────────────────┬─────────────────────┐\n",
      "│ min                 ┆ max                 │\n",
      "│ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ datetime[μs]        │\n",
      "╞═════════════════════╪═════════════════════╡\n",
      "│ 2022-01-01 01:00:00 ┆ 2022-12-31 23:00:00 │\n",
      "└─────────────────────┴─────────────────────┘\n",
      "test.parquet rows: 41653875 | utf8 remaining: ['station_id']\n",
      "shape: (1, 2)\n",
      "┌─────────────────────┬─────────────────────┐\n",
      "│ min                 ┆ max                 │\n",
      "│ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ datetime[μs]        │\n",
      "╞═════════════════════╪═════════════════════╡\n",
      "│ 2023-01-01 01:00:00 ┆ 2024-12-31 23:00:00 │\n",
      "└─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check (schemas, key columns)\n",
    "\n",
    "def quick_check(path: Path):\n",
    "    lf = pl.scan_parquet(path)\n",
    "    schema = lf.collect_schema()\n",
    "    cols = schema.names()\n",
    "    assert TARGET in cols and ID_COL in cols and TIME_COL in cols and \"station_idx\" in cols\n",
    "    # any Utf8?\n",
    "    utf8 = [c for c, dt in schema.items() if dt == pl.Utf8]\n",
    "    n = lf.select(pl.len()).collect().item()\n",
    "    tminmax = lf.select([pl.col(TIME_COL).min().alias(\"min\"), pl.col(TIME_COL).max().alias(\"max\")]).collect()\n",
    "    print(path.name, \"rows:\", n, \"| utf8 remaining:\", utf8)\n",
    "    print(tminmax)\n",
    "\n",
    "quick_check(train_out)\n",
    "quick_check(val_out)\n",
    "quick_check(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "773b1eae-217f-41b8-92fe-05f55f125370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled stations: 500\n"
     ]
    }
   ],
   "source": [
    "# Choose sequence settings + station sample\n",
    "\n",
    "SEQ_DIR = BASE / \"modeling/station/cnn_sequences\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "T = 24        # try 24 first; later repeat with 48\n",
    "H = 1         # 1-step ahead\n",
    "N_STATIONS = 500   # increase later if time allows\n",
    "SEED = 42\n",
    "\n",
    "# Get a station sample from TRAIN\n",
    "train_ids = (\n",
    "    pl.scan_parquet(train_out)\n",
    "    .select([\"station_idx\"])\n",
    "    .unique()\n",
    "    .collect()[\"station_idx\"]\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "station_sample = rng.choice(train_ids, size=min(N_STATIONS, len(train_ids)), replace=False).tolist()\n",
    "station_sample_set = set(station_sample)\n",
    "\n",
    "print(\"Sampled stations:\", len(station_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7502091c-7d1e-4ae7-9533-bfa52d148a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences and save as NPZ per split\n",
    "\n",
    "SEQ_DIR = BASE / \"modeling/station/cnn_sequences\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "T = 24\n",
    "H = 1\n",
    "\n",
    "MAX_SEQ_PER_SPLIT = 300_000   # cap total sequences per split (adjust up later)\n",
    "CHUNK_SIZE = 50_000           # sequences per chunk file\n",
    "SEED = 42\n",
    "\n",
    "def get_feature_cols(split_path: Path):\n",
    "    names = pl.scan_parquet(split_path).collect_schema().names()\n",
    "    return [c for c in names if c not in [TIME_COL, ID_COL, TARGET, \"station_idx\"]]\n",
    "\n",
    "def station_sequence_generator(split_path: Path, station_idx: int, feature_cols: list, T: int, H: int):\n",
    "    # Collect only one station at a time (bounded memory)\n",
    "    g = (\n",
    "        pl.scan_parquet(split_path)\n",
    "        .filter(pl.col(\"station_idx\") == station_idx)\n",
    "        .select([TIME_COL, \"station_idx\", TARGET] + feature_cols)\n",
    "        .collect()\n",
    "        .sort(TIME_COL)\n",
    "    )\n",
    "\n",
    "    if g.height < T + H:\n",
    "        return  # no sequences\n",
    "\n",
    "    y_arr = g[TARGET].to_numpy()\n",
    "    X_feat = g.select(feature_cols).to_numpy()\n",
    "\n",
    "    for i in range(T, g.height - H + 1):\n",
    "        X_seq = X_feat[i - T:i, :]\n",
    "        y_next = float(y_arr[i + H - 1])\n",
    "        yield X_seq, station_idx, y_next\n",
    "\n",
    "# def write_sequences_chunked(split_path: Path, out_prefix: str, station_idxs: list, T: int, H: int,\n",
    "#                             max_total: int, chunk_size: int, seed: int = 42):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     station_idxs = list(station_idxs)\n",
    "#     rng.shuffle(station_idxs)\n",
    "\n",
    "#     feature_cols = get_feature_cols(split_path)\n",
    "#     n_feat = len(feature_cols)\n",
    "#     print(split_path.name, \"| features:\", n_feat, \"| stations:\", len(station_idxs))\n",
    "\n",
    "#     X_buf, S_buf, y_buf = [], [], []\n",
    "#     total_written = 0\n",
    "#     chunk_id = 0\n",
    "\n",
    "#     def flush():\n",
    "#         nonlocal chunk_id, X_buf, S_buf, y_buf, total_written\n",
    "#         if not X_buf:\n",
    "#             return\n",
    "#         X = np.stack(X_buf).astype(np.float32)\n",
    "#         S = np.array(S_buf, dtype=np.int32)\n",
    "#         y = np.array(y_buf, dtype=np.float32)\n",
    "\n",
    "#         out_path = SEQ_DIR / f\"{out_prefix}_T{T}_chunk{chunk_id:03d}.npz\"\n",
    "#         np.savez_compressed(out_path, X=X, S=S, y=y, feature_cols=np.array(feature_cols))\n",
    "#         print(\"Wrote\", out_path.name, \"| sequences:\", X.shape[0])\n",
    "#         total_written += X.shape[0]\n",
    "#         chunk_id += 1\n",
    "#         X_buf, S_buf, y_buf = [], [], []\n",
    "\n",
    "#     for sid in station_idxs:\n",
    "#         if total_written >= max_total:\n",
    "#             break\n",
    "\n",
    "#         for X_seq, s, y_next in station_sequence_generator(split_path, sid, feature_cols, T, H):\n",
    "#             X_buf.append(X_seq)\n",
    "#             S_buf.append(s)\n",
    "#             y_buf.append(y_next)\n",
    "\n",
    "#             if len(X_buf) >= chunk_size:\n",
    "#                 flush()\n",
    "#                 if total_written >= max_total:\n",
    "#                     break\n",
    "\n",
    "#         if total_written >= max_total:\n",
    "#             break\n",
    "\n",
    "#     flush()\n",
    "#     print(f\"Done {out_prefix}: total_written={total_written}, chunks={chunk_id}, n_features={n_feat}\")\n",
    "\n",
    "# # Build chunked sequences for each split\n",
    "# write_sequences_chunked(train_out, \"train\", station_sample, T, H, MAX_SEQ_PER_SPLIT, CHUNK_SIZE, SEED)\n",
    "# write_sequences_chunked(val_out,   \"val\",   station_sample, T, H, MAX_SEQ_PER_SPLIT // 2, CHUNK_SIZE, SEED)\n",
    "# write_sequences_chunked(test_out,  \"test\",  station_sample, T, H, MAX_SEQ_PER_SPLIT // 2, CHUNK_SIZE, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3212ed3b-c92d-4c61-b38c-6771ccbf782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train chunks: 6\n",
      "Example file: train_T24_chunk000.npz | X: (50000, 24, 126)\n",
      "val chunks: 3\n",
      "Example file: val_T24_chunk000.npz | X: (50000, 24, 126)\n",
      "test chunks: 3\n",
      "Example file: test_T24_chunk000.npz | X: (50000, 24, 126)\n"
     ]
    }
   ],
   "source": [
    "# Quick inventory of what was written\n",
    "\n",
    "def list_chunks(prefix):\n",
    "    files = sorted(glob(str(SEQ_DIR / f\"{prefix}_T{T}_chunk*.npz\")))\n",
    "    print(prefix, \"chunks:\", len(files))\n",
    "    if files:\n",
    "        sample = np.load(files[0], allow_pickle=True)\n",
    "        print(\"Example file:\", Path(files[0]).name, \"| X:\", sample[\"X\"].shape)\n",
    "\n",
    "list_chunks(\"train\")\n",
    "list_chunks(\"val\")\n",
    "list_chunks(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdd41769-70e9-44da-a54a-1b54d6e050ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (300000, 126) (100000, 126) (100000, 126)\n",
      "NaN rate (train/val/test): 0.018050925925925925 0.012362698412698412 0.01719547619047619\n"
     ]
    }
   ],
   "source": [
    "# Sample rows + build X/y\n",
    "\n",
    "ROW_N_TRAIN = 300_000\n",
    "ROW_N_VAL   = 100_000\n",
    "ROW_N_TEST  = 100_000\n",
    "SEED = 42\n",
    "\n",
    "def row_sample_to_pandas(path, n, seed=42):\n",
    "    lf = pl.scan_parquet(path).with_row_index(\"row_nr\")\n",
    "    mod_base = max(n * 3, 1)\n",
    "    df = (\n",
    "        lf.with_columns((((pl.col(\"row_nr\") * 1103515245 + seed) & 0x7FFFFFFF) % mod_base).alias(\"_k\"))\n",
    "          .filter(pl.col(\"_k\") < n)\n",
    "          .drop([\"row_nr\", \"_k\"])\n",
    "          .limit(n)\n",
    "          .collect()\n",
    "          .to_pandas()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def prep_tabular(df):\n",
    "    y = df[TARGET].astype(\"float32\")\n",
    "    X = df.drop(columns=[TARGET, ID_COL, TIME_COL], errors=\"ignore\")\n",
    "\n",
    "    # Drop any datetime-like columns defensively\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        X = X.drop(columns=dt_cols)\n",
    "\n",
    "    # ensure numeric dtypes only (everything should already be numeric/codes)\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "tr = row_sample_to_pandas(train_out, ROW_N_TRAIN, SEED)\n",
    "va = row_sample_to_pandas(val_out,   ROW_N_VAL,   SEED)\n",
    "te = row_sample_to_pandas(test_out,  ROW_N_TEST,  SEED)\n",
    "\n",
    "X_tr, y_tr = prep_tabular(tr)\n",
    "X_va, y_va = prep_tabular(va)\n",
    "X_te, y_te = prep_tabular(te)\n",
    "\n",
    "def nan_rate(X):\n",
    "    return float(np.isnan(X.to_numpy()).mean())\n",
    "\n",
    "print(\"Shapes:\", X_tr.shape, X_va.shape, X_te.shape)\n",
    "print(\"NaN rate (train/val/test):\", nan_rate(X_tr), nan_rate(X_va), nan_rate(X_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb25ba96-007e-40d4-9813-d430371dc007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB (station hourly, sampled rows)\n",
      "VAL  RMSE: 0.638553394150442\n",
      "TEST RMSE: 0.6521865668536464\n",
      "TEST MAE : 0.4104449759257502\n",
      "TEST MAPE: 0.360338323659901\n",
      "TEST R²  : 0.024983362006145482\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoostingRegressor\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hgb.fit(X_tr, y_tr)\n",
    "\n",
    "val_pred = hgb.predict(X_va)\n",
    "test_pred = hgb.predict(X_te)\n",
    "\n",
    "def safe_mape(y_true, y_pred, eps=1.0):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), eps))\n",
    "\n",
    "print(\"HGB (station hourly, sampled rows)\")\n",
    "print(\"VAL  RMSE:\", (mean_squared_error(y_va, val_pred) ** 0.5))\n",
    "print(\"TEST RMSE:\", (mean_squared_error(y_te, test_pred) ** 0.5))\n",
    "print(\"TEST MAE :\", mean_absolute_error(y_te, test_pred))\n",
    "print(\"TEST MAPE:\", safe_mape(y_te, test_pred))\n",
    "print(\"TEST R²  :\", r2_score(y_te, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe3ae268-b722-4dac-9879-4c4896018905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define improved chunk writer (balanced + includes pred_time)\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "CNN_DIR = BASE / \"modeling/station/hourly_cnn_ready\"\n",
    "SEQ_DIR = BASE / \"modeling/station/cnn_sequences_v2\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_pq = CNN_DIR / \"train.parquet\"\n",
    "val_pq   = CNN_DIR / \"val.parquet\"\n",
    "test_pq  = CNN_DIR / \"test.parquet\"\n",
    "\n",
    "TIME_COL = \"pred_time\"\n",
    "TARGET = \"y\"\n",
    "ID_COL = \"station_id\"\n",
    "\n",
    "T = 24\n",
    "H = 1\n",
    "SEED = 42\n",
    "\n",
    "# caps\n",
    "MAX_SEQ_PER_SPLIT = 300_000\n",
    "CHUNK_SIZE = 50_000\n",
    "MAX_SEQ_PER_STATION = 1_000   # key fix for diversity (adjust 500–2000)\n",
    "\n",
    "def get_feature_cols(split_path: Path):\n",
    "    names = pl.scan_parquet(split_path).collect_schema().names()\n",
    "\n",
    "    drop = {TIME_COL, ID_COL, TARGET, \"station_idx\", \"date\"}  # drop 'date' explicitly\n",
    "    feat_cols = [c for c in names if c not in drop]\n",
    "\n",
    "    # defensive: ensure no datetime columns slip through\n",
    "    schema = pl.scan_parquet(split_path).collect_schema()\n",
    "    for c in list(feat_cols):\n",
    "        if str(schema[c]).startswith(\"Datetime\") or str(schema[c]).startswith(\"Date\"):\n",
    "            feat_cols.remove(c)\n",
    "\n",
    "    return feat_cols\n",
    "\n",
    "def station_sequence_generator(split_path: Path, station_idx: int, feature_cols: list, T: int, H: int, max_per_station: int):\n",
    "    g = (\n",
    "        pl.scan_parquet(split_path)\n",
    "        .filter(pl.col(\"station_idx\") == station_idx)\n",
    "        .select([TIME_COL, \"station_idx\", TARGET] + feature_cols)\n",
    "        .collect()\n",
    "        .sort(TIME_COL)\n",
    "    )\n",
    "\n",
    "    if g.height < T + H:\n",
    "        return\n",
    "\n",
    "    times = g[TIME_COL].to_numpy()\n",
    "    y_arr = g[TARGET].to_numpy()\n",
    "    X_feat = g.select(feature_cols).to_numpy()\n",
    "\n",
    "    n_written = 0\n",
    "    for i in range(T, g.height - H + 1):\n",
    "        X_seq = X_feat[i - T:i, :]\n",
    "        y_next = float(y_arr[i + H - 1])\n",
    "        pred_time = times[i]  # target time t\n",
    "\n",
    "        yield X_seq, station_idx, y_next, pred_time\n",
    "\n",
    "        n_written += 1\n",
    "        if n_written >= max_per_station:\n",
    "            break\n",
    "\n",
    "def write_sequences_chunked(split_path: Path, out_prefix: str, station_idxs: list, T: int, H: int,\n",
    "                            max_total: int, chunk_size: int, max_per_station: int, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    station_idxs = list(station_idxs)\n",
    "    rng.shuffle(station_idxs)\n",
    "\n",
    "    feature_cols = get_feature_cols(split_path)\n",
    "    n_feat = len(feature_cols)\n",
    "    print(out_prefix, \"| features:\", n_feat, \"| stations:\", len(station_idxs))\n",
    "\n",
    "    X_buf, S_buf, y_buf, t_buf = [], [], [], []\n",
    "    total_written = 0\n",
    "    chunk_id = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal chunk_id, X_buf, S_buf, y_buf, t_buf, total_written\n",
    "        if not X_buf:\n",
    "            return\n",
    "        X = np.stack(X_buf).astype(np.float32)\n",
    "        S = np.array(S_buf, dtype=np.int32)\n",
    "        y = np.array(y_buf, dtype=np.float32)\n",
    "        pt = np.array(t_buf)  # numpy datetime64\n",
    "\n",
    "        out_path = SEQ_DIR / f\"{out_prefix}_T{T}_chunk{chunk_id:03d}.npz\"\n",
    "        np.savez_compressed(out_path, X=X, S=S, y=y, pred_time=pt, feature_cols=np.array(feature_cols))\n",
    "        print(\"Wrote\", out_path.name, \"| sequences:\", X.shape[0])\n",
    "        total_written += X.shape[0]\n",
    "        chunk_id += 1\n",
    "        X_buf, S_buf, y_buf, t_buf = [], [], [], []\n",
    "\n",
    "    for sid in station_idxs:\n",
    "        if total_written >= max_total:\n",
    "            break\n",
    "\n",
    "        for X_seq, s, y_next, pred_time in station_sequence_generator(split_path, sid, feature_cols, T, H, max_per_station):\n",
    "            X_buf.append(X_seq)\n",
    "            S_buf.append(int(s))\n",
    "            y_buf.append(float(y_next))\n",
    "            t_buf.append(pred_time)\n",
    "\n",
    "            if len(X_buf) >= chunk_size:\n",
    "                flush()\n",
    "                if total_written >= max_total:\n",
    "                    break\n",
    "\n",
    "        if total_written >= max_total:\n",
    "            break\n",
    "\n",
    "    flush()\n",
    "    print(f\"Done {out_prefix}: total_written={total_written}, chunks={chunk_id}, n_features={n_feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "723ce2b0-b15a-422b-83e4-0db22c0afc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | features: 125 | stations: 500\n",
      "Wrote train_T24_chunk000.npz | sequences: 50000\n",
      "Wrote train_T24_chunk001.npz | sequences: 50000\n",
      "Wrote train_T24_chunk002.npz | sequences: 50000\n",
      "Wrote train_T24_chunk003.npz | sequences: 50000\n",
      "Wrote train_T24_chunk004.npz | sequences: 50000\n",
      "Wrote train_T24_chunk005.npz | sequences: 50000\n",
      "Done train: total_written=300000, chunks=6, n_features=125\n",
      "val | features: 125 | stations: 500\n",
      "Wrote val_T24_chunk000.npz | sequences: 50000\n",
      "Wrote val_T24_chunk001.npz | sequences: 50000\n",
      "Wrote val_T24_chunk002.npz | sequences: 50000\n",
      "Done val: total_written=150000, chunks=3, n_features=125\n",
      "test | features: 125 | stations: 500\n",
      "Wrote test_T24_chunk000.npz | sequences: 50000\n",
      "Wrote test_T24_chunk001.npz | sequences: 50000\n",
      "Wrote test_T24_chunk002.npz | sequences: 50000\n",
      "Done test: total_written=150000, chunks=3, n_features=125\n"
     ]
    }
   ],
   "source": [
    "# Regenerate train/val/test chunks\n",
    "\n",
    "write_sequences_chunked(train_pq, \"train\", station_sample, T, H, MAX_SEQ_PER_SPLIT, CHUNK_SIZE, MAX_SEQ_PER_STATION, SEED)\n",
    "write_sequences_chunked(val_pq,   \"val\",   station_sample, T, H, MAX_SEQ_PER_SPLIT // 2, CHUNK_SIZE, MAX_SEQ_PER_STATION, SEED)\n",
    "write_sequences_chunked(test_pq,  \"test\",  station_sample, T, H, MAX_SEQ_PER_SPLIT // 2, CHUNK_SIZE, MAX_SEQ_PER_STATION, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d47609f-592a-495d-81e2-e650d6b72337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['X', 'S', 'y', 'pred_time', 'feature_cols']\n",
      "X: (50000, 24, 125)\n",
      "Unique stations in first chunk: 50\n",
      "Has pred_time: True\n",
      "Has feature_cols: True\n",
      "Contains 'date' feature: False\n"
     ]
    }
   ],
   "source": [
    "# Quick re-check (diversity + date removed + pred_time present)\n",
    "\n",
    "def load_first(prefix):\n",
    "    files = sorted(glob(str(SEQ_DIR / f\"{prefix}_T{T}_chunk*.npz\")))\n",
    "    d = np.load(files[0], allow_pickle=True)\n",
    "    return d\n",
    "\n",
    "d = load_first(\"train\")\n",
    "print(\"Keys:\", list(d.keys()))\n",
    "print(\"X:\", d[\"X\"].shape)\n",
    "print(\"Unique stations in first chunk:\", len(np.unique(d[\"S\"])))\n",
    "print(\"Has pred_time:\", \"pred_time\" in d.files)\n",
    "print(\"Has feature_cols:\", \"feature_cols\" in d.files)\n",
    "print(\"Contains 'date' feature:\", \"date\" in list(d[\"feature_cols\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "195e41d6-d418-4923-b51b-ae8b306bbcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPZ station_idx: 1569\n",
      "NPZ pred_time: 2017-01-06T05:00:00.000000\n",
      "NPZ y: 0.0\n",
      "Source match rows: 1\n",
      "shape: (1, 3)\n",
      "┌─────────────────────┬─────────────┬─────┐\n",
      "│ pred_time           ┆ station_idx ┆ y   │\n",
      "│ ---                 ┆ ---         ┆ --- │\n",
      "│ datetime[μs]        ┆ i32         ┆ f32 │\n",
      "╞═════════════════════╪═════════════╪═════╡\n",
      "│ 2017-01-06 05:00:00 ┆ 1569        ┆ 0.0 │\n",
      "└─────────────────────┴─────────────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "# Alignment check becomes unambiguous (uses pred_time)\n",
    "\n",
    "# load one train chunk\n",
    "files = sorted(glob(str(SEQ_DIR / f\"train_T{T}_chunk*.npz\")))\n",
    "d = np.load(files[0], allow_pickle=True)\n",
    "\n",
    "X = d[\"X\"]\n",
    "S = d[\"S\"]\n",
    "y = d[\"y\"]\n",
    "pt = d[\"pred_time\"]\n",
    "\n",
    "i = 100  # pick any index\n",
    "sid = int(S[i])\n",
    "pred_time = pt[i]\n",
    "y_npz = float(y[i])\n",
    "\n",
    "g = (\n",
    "    pl.scan_parquet(train_pq)\n",
    "    .filter((pl.col(\"station_idx\") == sid) & (pl.col(TIME_COL) == pl.lit(pred_time)))\n",
    "    .select([TIME_COL, \"station_idx\", TARGET])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"NPZ station_idx:\", sid)\n",
    "print(\"NPZ pred_time:\", pred_time)\n",
    "print(\"NPZ y:\", y_npz)\n",
    "print(\"Source match rows:\", g.shape[0])\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83914149-70ae-4474-9a69-c005858393c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB baseline aligned to station_sample\n",
      "VAL  RMSE: 1.5562880875275398\n",
      "TEST RMSE: 1.6967175269325334\n",
      "TEST MAE : 1.0317997285750538\n",
      "TEST MAPE: 0.5714711954635572\n",
      "TEST R²  : 0.560791202878895\n"
     ]
    }
   ],
   "source": [
    "BASE = \"/Users/zoltanjelovich/Documents/ISEG/MFW/data\"\n",
    "TRAIN_PQ = f\"{BASE}/modeling/station/hourly_cnn_ready/train.parquet\"\n",
    "VAL_PQ   = f\"{BASE}/modeling/station/hourly_cnn_ready/val.parquet\"\n",
    "TEST_PQ  = f\"{BASE}/modeling/station/hourly_cnn_ready/test.parquet\"\n",
    "\n",
    "TIME_COL = \"pred_time\"\n",
    "TARGET = \"y\"\n",
    "ID_COL = \"station_id\"\n",
    "\n",
    "station_sample_set = set(station_sample)\n",
    "\n",
    "ROW_N_TRAIN = 300_000\n",
    "ROW_N_VAL   = 100_000\n",
    "ROW_N_TEST  = 100_000\n",
    "SEED = 42\n",
    "\n",
    "def sample_rows_from_stations(path, station_idxs, n_rows, seed=42):\n",
    "    lf = (\n",
    "        pl.scan_parquet(path)\n",
    "        .filter(pl.col(\"station_idx\").is_in(list(station_idxs)))\n",
    "        .with_row_index(\"row_nr\")\n",
    "    )\n",
    "    mod_base = max(n_rows * 5, 1)\n",
    "    df = (\n",
    "        lf.with_columns((((pl.col(\"row_nr\") * 1103515245 + seed) & 0x7FFFFFFF) % mod_base).alias(\"_k\"))\n",
    "          .filter(pl.col(\"_k\") < n_rows)\n",
    "          .drop([\"row_nr\", \"_k\"])\n",
    "          .limit(n_rows)\n",
    "          .collect()\n",
    "          .to_pandas()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def prep_tabular(df):\n",
    "    y = df[TARGET].astype(\"float32\")\n",
    "    X = df.drop(columns=[TARGET, ID_COL, TIME_COL], errors=\"ignore\")\n",
    "\n",
    "    # Drop datetime-like columns defensively\n",
    "    dt_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    if dt_cols:\n",
    "        X = X.drop(columns=dt_cols)\n",
    "\n",
    "    # Ensure numeric\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    return X, y\n",
    "\n",
    "tr = sample_rows_from_stations(TRAIN_PQ, station_sample_set, ROW_N_TRAIN, SEED)\n",
    "va = sample_rows_from_stations(VAL_PQ,   station_sample_set, ROW_N_VAL,   SEED)\n",
    "te = sample_rows_from_stations(TEST_PQ,  station_sample_set, ROW_N_TEST,  SEED)\n",
    "\n",
    "X_tr, y_tr = prep_tabular(tr)\n",
    "X_va, y_va = prep_tabular(va)\n",
    "X_te, y_te = prep_tabular(te)\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "hgb.fit(X_tr, y_tr)\n",
    "\n",
    "val_pred = hgb.predict(X_va)\n",
    "test_pred = hgb.predict(X_te)\n",
    "\n",
    "def safe_mape(y_true, y_pred, eps=1.0):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), eps)))\n",
    "\n",
    "print(\"HGB baseline aligned to station_sample\")\n",
    "print(\"VAL  RMSE:\", float(mean_squared_error(y_va, val_pred) ** 0.5))\n",
    "print(\"TEST RMSE:\", float(mean_squared_error(y_te, test_pred) ** 0.5))\n",
    "print(\"TEST MAE :\", float(mean_absolute_error(y_te, test_pred)))\n",
    "print(\"TEST MAPE:\", safe_mape(y_te, test_pred))\n",
    "print(\"TEST R²  :\", float(r2_score(y_te, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae688f89-46c6-4406-b7a2-5059898d40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "Chunks | train: 6 | val: 3 | test: 3\n",
      "train example: train_T24_chunk000.npz | X: (50000, 24, 125) | S dtype: int32 | y dtype: float32 | unique stations: 50\n",
      "val example: val_T24_chunk000.npz | X: (50000, 24, 125) | S dtype: int32 | y dtype: float32 | unique stations: 50\n",
      "test example: test_T24_chunk000.npz | X: (50000, 24, 125) | S dtype: int32 | y dtype: float32 | unique stations: 50\n",
      "✔ feature_cols identical across train/val/test | n_features: 125\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths, and a fast NPZ inventory check\n",
    "\n",
    "# Repro/determinism\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "BASE = Path(\"/Users/zoltanjelovich/Documents/ISEG/MFW/data\")\n",
    "SEQ_DIR = BASE / \"modeling/station/cnn_sequences_v2\"\n",
    "\n",
    "T = 24\n",
    "train_files = sorted(glob(str(SEQ_DIR / f\"train_T{T}_chunk*.npz\")))\n",
    "val_files   = sorted(glob(str(SEQ_DIR / f\"val_T{T}_chunk*.npz\")))\n",
    "test_files  = sorted(glob(str(SEQ_DIR / f\"test_T{T}_chunk*.npz\")))\n",
    "\n",
    "assert len(train_files) > 0 and len(val_files) > 0 and len(test_files) > 0, \"No NPZ chunks found\"\n",
    "print(\"Chunks | train:\", len(train_files), \"| val:\", len(val_files), \"| test:\", len(test_files))\n",
    "\n",
    "def peek_npz(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    X, S, y = d[\"X\"], d[\"S\"], d[\"y\"]\n",
    "    assert X.ndim == 3, \"X must be (N,T,F)\"\n",
    "    assert S.ndim == 1 and y.ndim == 1\n",
    "    assert X.shape[0] == S.shape[0] == y.shape[0]\n",
    "    assert \"pred_time\" in d.files, \"pred_time missing (needed for traceability)\"\n",
    "    assert \"feature_cols\" in d.files, \"feature_cols missing (needed for consistent ordering)\"\n",
    "    return X.shape, S.dtype, y.dtype, len(np.unique(S)), d[\"feature_cols\"]\n",
    "\n",
    "for name, files in [(\"train\", train_files), (\"val\", val_files), (\"test\", test_files)]:\n",
    "    shp, sd, yd, nunq, fcols = peek_npz(files[0])\n",
    "    print(f\"{name} example:\", Path(files[0]).name, \"| X:\", shp, \"| S dtype:\", sd, \"| y dtype:\", yd, \"| unique stations:\", nunq)\n",
    "\n",
    "# Ensure identical feature ordering across splits\n",
    "train_fcols = np.load(train_files[0], allow_pickle=True)[\"feature_cols\"].tolist()\n",
    "val_fcols   = np.load(val_files[0], allow_pickle=True)[\"feature_cols\"].tolist()\n",
    "test_fcols  = np.load(test_files[0], allow_pickle=True)[\"feature_cols\"].tolist()\n",
    "\n",
    "assert train_fcols == val_fcols == test_fcols, \"Feature order mismatch across splits!\"\n",
    "print(\"✔ feature_cols identical across train/val/test | n_features:\", len(train_fcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4813a08f-3ebf-4b75-849e-f7fab5c0c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust NPZ chunk dataset (lazy loading, NaN/Inf handling)\n",
    "\n",
    "class NPZChunkDataset(Dataset):\n",
    "    def __init__(self, npz_files, return_time=False):\n",
    "        self.files = list(npz_files)\n",
    "        self.return_time = return_time\n",
    "\n",
    "        # Build global index: map global row\n",
    "        self.counts = []\n",
    "        for fp in self.files:\n",
    "            with np.load(fp, allow_pickle=True) as d:\n",
    "                self.counts.append(int(d[\"X\"].shape[0]))\n",
    "        self.offsets = np.cumsum([0] + self.counts)  # length = n_files + 1\n",
    "        self.n = int(self.offsets[-1])\n",
    "\n",
    "        # Cache one currently-open file to reduce IO overhead\n",
    "        self._cache_file_idx = None\n",
    "        self._cache_data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def _load_file(self, file_idx):\n",
    "        if self._cache_file_idx == file_idx and self._cache_data is not None:\n",
    "            return self._cache_data\n",
    "        # close previous\n",
    "        self._cache_data = np.load(self.files[file_idx], allow_pickle=True)\n",
    "        self._cache_file_idx = file_idx\n",
    "        return self._cache_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # locate file\n",
    "        file_idx = int(np.searchsorted(self.offsets, idx, side=\"right\") - 1)\n",
    "        local_idx = int(idx - self.offsets[file_idx])\n",
    "\n",
    "        d = self._load_file(file_idx)\n",
    "\n",
    "        X = d[\"X\"][local_idx]  # (T,F)\n",
    "        s = d[\"S\"][local_idx]\n",
    "        y = d[\"y\"][local_idx]\n",
    "\n",
    "        # Replace NaN/Inf in X with 0.0\n",
    "        if not np.isfinite(X).all():\n",
    "            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # Convert\n",
    "        X = torch.from_numpy(X).float()          # (T,F)\n",
    "        s = torch.tensor(int(s)).long()          # embedding index\n",
    "        y = torch.tensor(float(y)).float()\n",
    "\n",
    "        if self.return_time:\n",
    "            pt = d[\"pred_time\"][local_idx]\n",
    "            return X, s, y, pt\n",
    "        return X, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "763b4053-1f4d-4160-aee2-3a3547fb75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean/std | n_features: 125\n",
      "std min/max: 9.999999974752427e-07 2045011.375\n",
      "Wrote: /Users/zoltanjelovich/Documents/ISEG/MFW/data/modeling/station/cnn_sequences_v2/norm_stats_T24.npz\n"
     ]
    }
   ],
   "source": [
    "# Feature standardization\n",
    "\n",
    "def compute_train_norm_stats(train_files, max_chunks=None):\n",
    "    sum_x = None\n",
    "    sum_x2 = None\n",
    "    n_total = 0\n",
    "\n",
    "    files = train_files if max_chunks is None else train_files[:max_chunks]\n",
    "    for fp in files:\n",
    "        d = np.load(fp, allow_pickle=True)\n",
    "        X = d[\"X\"]\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # Flatten across N*T\n",
    "        X2d = X.reshape(-1, X.shape[-1]).astype(np.float64)\n",
    "        if sum_x is None:\n",
    "            sum_x = X2d.sum(axis=0)\n",
    "            sum_x2 = (X2d ** 2).sum(axis=0)\n",
    "        else:\n",
    "            sum_x += X2d.sum(axis=0)\n",
    "            sum_x2 += (X2d ** 2).sum(axis=0)\n",
    "\n",
    "        n_total += X2d.shape[0]\n",
    "\n",
    "    mean = sum_x / n_total\n",
    "    var = (sum_x2 / n_total) - (mean ** 2)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "# Compute stats from train only\n",
    "mean, std = compute_train_norm_stats(train_files, max_chunks=None)\n",
    "print(\"Computed mean/std | n_features:\", len(mean))\n",
    "print(\"std min/max:\", float(std.min()), float(std.max()))\n",
    "assert np.all(std > 0), \"std must be > 0\"\n",
    "\n",
    "# Store for reuse\n",
    "NORM_PATH = SEQ_DIR / f\"norm_stats_T{T}.npz\"\n",
    "np.savez_compressed(NORM_PATH, mean=mean, std=std, feature_cols=np.array(train_fcols))\n",
    "print(\"Wrote:\", NORM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2256492-fc56-46b8-bc4f-5ccd5323f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X mean: 0.00304386205971241 X std: 0.943386435508728\n",
      "Shapes: torch.Size([256, 24, 125]) torch.Size([256]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Normalized dataset wrapper\n",
    "\n",
    "class NormalizedDataset(Dataset):\n",
    "    def __init__(self, base_ds, mean, std):\n",
    "        self.base = base_ds\n",
    "        self.mean = torch.from_numpy(mean).float()\n",
    "        self.std = torch.from_numpy(std).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, s, y = self.base[idx]\n",
    "        X = (X - self.mean) / self.std\n",
    "        X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return X, s, y\n",
    "\n",
    "# Recreate base datasets\n",
    "train_ds = NPZChunkDataset(train_files)\n",
    "val_ds   = NPZChunkDataset(val_files)\n",
    "test_ds  = NPZChunkDataset(test_files)\n",
    "\n",
    "# Load normalization stats\n",
    "norm = np.load(NORM_PATH)\n",
    "mean = norm[\"mean\"]\n",
    "std  = norm[\"std\"]\n",
    "\n",
    "# Create normalized datasets\n",
    "train_ds_n = NormalizedDataset(train_ds, mean, std)\n",
    "val_ds_n   = NormalizedDataset(val_ds, mean, std)\n",
    "test_ds_n  = NormalizedDataset(test_ds, mean, std)\n",
    "\n",
    "# Recreate loaders\n",
    "train_loader = DataLoader(train_ds_n, batch_size=256, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds_n, batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds_n, batch_size=256, shuffle=False)\n",
    "\n",
    "# Quick sanity batch\n",
    "Xb, sb, yb = next(iter(train_loader))\n",
    "print(\"X mean:\", float(Xb.mean()), \"X std:\", float(Xb.std()))\n",
    "print(\"Shapes:\", Xb.shape, sb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "607919c5-1f25-410b-be7c-69269ba23633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN–LSTM with station embedding\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_stations: int,\n",
    "        emb_dim: int = 16,\n",
    "        conv_channels: int = 64,\n",
    "        conv_kernel: int = 3,\n",
    "        lstm_hidden: int = 64,\n",
    "        lstm_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.station_emb = nn.Embedding(n_stations, emb_dim)\n",
    "\n",
    "        # Conv over time dimension\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_features, out_channels=conv_channels, kernel_size=conv_kernel, padding=conv_kernel//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=conv_kernel, padding=conv_kernel//2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # LSTM expects (B, T, C)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, station_idx):\n",
    "        x = x_seq.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        h_last = out[:, -1, :]\n",
    "\n",
    "        s_emb = self.station_emb(station_idx)\n",
    "        z = torch.cat([h_last, s_emb], dim=1)\n",
    "\n",
    "        yhat = self.head(z).squeeze(1)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "676a1337-9767-4b1e-84d7-06cc7d610a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 125 | n_stations: 2690\n",
      "Model parameters: 117985\n",
      "yhat shape: torch.Size([256]) | yhat dtype: torch.float32\n",
      "yhat summary: -0.20163452625274658 0.09736275672912598 0.4507068991661072\n",
      "✔ Forward pass OK\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model + forward pass\n",
    "\n",
    "Xb, sb, yb = next(iter(train_loader))\n",
    "n_features = Xb.shape[-1]\n",
    "n_stations = int(sb.max().item()) + 1\n",
    "\n",
    "model = CNNLSTM(\n",
    "    n_features=n_features,\n",
    "    n_stations=n_stations,\n",
    "    emb_dim=16,\n",
    "    conv_channels=64,\n",
    "    conv_kernel=3,\n",
    "    lstm_hidden=64,\n",
    "    lstm_layers=1,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"n_features:\", n_features, \"| n_stations:\", n_stations)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# Forward pass\n",
    "Xb = Xb.to(DEVICE)\n",
    "sb = sb.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    yhat = model(Xb, sb)\n",
    "\n",
    "print(\"yhat shape:\", yhat.shape, \"| yhat dtype:\", yhat.dtype)\n",
    "print(\"yhat summary:\", float(yhat.min()), float(yhat.mean()), float(yhat.max()))\n",
    "assert yhat.shape == yb.to(DEVICE).shape, \"Output shape must match y shape\"\n",
    "print(\"✔ Forward pass OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac99ff68-4be1-4faf-8ba8-c8320bb0571c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss (single batch): 6.798551082611084\n"
     ]
    }
   ],
   "source": [
    "# Quick loss computation sanity check\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "yb = yb.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = criterion(yhat, yb)\n",
    "print(\"MSE loss (single batch):\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "968f70f8-afb9-46f5-8340-61d3eddfe35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics + one-epoch train/eval helpers\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def safe_mape(y_true, y_pred, eps=1.0):\n",
    "    # eps=1 avoids exploding MAPE when y is 0-heavy\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device=DEVICE, max_batches=None, log_every=50):\n",
    "    model.eval()\n",
    "    ys, yhats = [], []\n",
    "    for b, batch in enumerate(loader):\n",
    "        if (b + 1) % log_every == 0:\n",
    "            print(f\"  val batch {b+1}\")\n",
    "\n",
    "        if max_batches is not None and b >= max_batches:\n",
    "            break\n",
    "\n",
    "        X, s, y = batch\n",
    "        X, s = X.to(device), s.to(device)\n",
    "        yhat = model(X, s).detach().cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        yhats.append(yhat)\n",
    "\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(yhats)\n",
    "    return {\"rmse\": rmse(y_true, y_pred), \"mae\": mae(y_true, y_pred), \"mape\": safe_mape(y_true, y_pred)}\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device=DEVICE, max_batches=None, log_every=50):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for b, batch in enumerate(loader):\n",
    "        if (b + 1) % log_every == 0:\n",
    "            print(f\"  train batch {b+1}\")\n",
    "\n",
    "        if max_batches is not None and b >= max_batches:\n",
    "            break\n",
    "\n",
    "        X, s, y = batch\n",
    "        X, s, y = X.to(device), s.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        yhat = model(X, s)\n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += float(loss.item()) * X.shape[0]\n",
    "        n += X.shape[0]\n",
    "\n",
    "    return running / max(n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d26a40cf-062c-4a0c-9f69-8632ee2aff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split': 'train', 'files_checked': 2, 'min': 17, 'max': 2636, 'n_unique_sample': 10}\n",
      "{'split': 'val', 'files_checked': 2, 'min': 93, 'max': 2657, 'n_unique_sample': 10}\n",
      "{'split': 'test', 'files_checked': 2, 'min': 129, 'max': 2657, 'n_unique_sample': 10}\n"
     ]
    }
   ],
   "source": [
    "def quick_station_idx_stats(seq_dir: Path, split: str, T: int, n_chunks=2):\n",
    "    files = sorted(seq_dir.glob(f\"{split}_T{T}_chunk*.npz\"))\n",
    "    assert len(files) > 0, f\"No NPZ chunks found for {split} T={T} in {seq_dir}\"\n",
    "    files = files[:n_chunks]\n",
    "\n",
    "    mins, maxs, uniq = [], [], set()\n",
    "    for fp in files:\n",
    "        d = np.load(fp, allow_pickle=False)\n",
    "        S = d[\"S\"]\n",
    "        mins.append(int(S.min()))\n",
    "        maxs.append(int(S.max()))\n",
    "        uniq.update(np.unique(S[:5000]).tolist())  # sample uniques cheaply\n",
    "\n",
    "    return {\n",
    "        \"split\": split,\n",
    "        \"files_checked\": len(files),\n",
    "        \"min\": min(mins),\n",
    "        \"max\": max(maxs),\n",
    "        \"n_unique_sample\": len(uniq),\n",
    "    }\n",
    "\n",
    "T = 24\n",
    "print(quick_station_idx_stats(SEQ_DIR, \"train\", T, n_chunks=2))\n",
    "print(quick_station_idx_stats(SEQ_DIR, \"val\",   T, n_chunks=2))\n",
    "print(quick_station_idx_stats(SEQ_DIR, \"test\",  T, n_chunks=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8dfec25-7157-4544-9170-6f158edf7bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max station_idx: 2657 | n_stations for embedding: 2658\n",
      "n_features: 125\n",
      "Embedding num_embeddings: 2658\n",
      "station_idx within embedding range for this chunk\n"
     ]
    }
   ],
   "source": [
    "# Get max station_idx across splits (check a few chunks each)\n",
    "stats_train = quick_station_idx_stats(SEQ_DIR, \"train\", T, n_chunks=3)\n",
    "stats_val   = quick_station_idx_stats(SEQ_DIR, \"val\",   T, n_chunks=2)\n",
    "stats_test  = quick_station_idx_stats(SEQ_DIR, \"test\",  T, n_chunks=2)\n",
    "\n",
    "max_sid = max(stats_train[\"max\"], stats_val[\"max\"], stats_test[\"max\"])\n",
    "n_stations = max_sid + 1\n",
    "print(\"max station_idx:\", max_sid, \"| n_stations for embedding:\", n_stations)\n",
    "\n",
    "# n_features should come from feature_cols in the NPZ to avoid mismatch after restart\n",
    "ex = np.load(sorted(SEQ_DIR.glob(f\"train_T{T}_chunk*.npz\"))[0], allow_pickle=False)\n",
    "n_features = int(ex[\"X\"].shape[-1])\n",
    "print(\"n_features:\", n_features)\n",
    "\n",
    "# Re-instantiate model\n",
    "model = CNNLSTM(\n",
    "    n_features=n_features,\n",
    "    n_stations=n_stations,\n",
    "    emb_dim=16,\n",
    "    conv_channels=32,\n",
    "    lstm_hidden=64,\n",
    "    lstm_layers=1,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# Quick sanity: embedding size\n",
    "print(\"Embedding num_embeddings:\", model.station_emb.num_embeddings)\n",
    "\n",
    "# Hard assert using one real chunk\n",
    "S_check = ex[\"S\"]\n",
    "assert int(S_check.max()) < model.station_emb.num_embeddings, \"Embedding still too small!\"\n",
    "print(\"station_idx within embedding range for this chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53393936-f712-4bd5-bd54-59c942fe1f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 17 2689 files: 6\n",
      "val 93 2689 files: 3\n",
      "test 129 2689 files: 3\n"
     ]
    }
   ],
   "source": [
    "def station_idx_range_all_chunks(seq_dir: Path, split: str, T: int):\n",
    "    files = sorted(seq_dir.glob(f\"{split}_T{T}_chunk*.npz\"))\n",
    "    smin, smax = None, None\n",
    "    bad = []\n",
    "\n",
    "    for fp in files:\n",
    "        d = np.load(fp, allow_pickle=False)\n",
    "        S = d[\"S\"]\n",
    "        mn, mx = int(S.min()), int(S.max())\n",
    "\n",
    "        if smin is None or mn < smin: smin = mn\n",
    "        if smax is None or mx > smax: smax = mx\n",
    "\n",
    "        if mn < 0:\n",
    "            bad.append((fp.name, mn, mx))\n",
    "\n",
    "    return {\"min\": smin, \"max\": smax, \"n_files\": len(files), \"neg_chunks\": bad}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    r = station_idx_range_all_chunks(SEQ_DIR, split, T)\n",
    "    print(split, r[\"min\"], r[\"max\"], \"files:\", r[\"n_files\"])\n",
    "    if r[\"neg_chunks\"]:\n",
    "        print(\"  negative station_idx found in:\", r[\"neg_chunks\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9781063-6767-4c71-a46c-c3c892f6339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_stations: 2690\n",
      "Embedding num_embeddings: 2690\n"
     ]
    }
   ],
   "source": [
    "# Compute from full scan\n",
    "r_train = station_idx_range_all_chunks(SEQ_DIR, \"train\", T)\n",
    "r_val   = station_idx_range_all_chunks(SEQ_DIR, \"val\",   T)\n",
    "r_test  = station_idx_range_all_chunks(SEQ_DIR, \"test\",  T)\n",
    "\n",
    "n_stations = max(r_train[\"max\"], r_val[\"max\"], r_test[\"max\"]) + 1\n",
    "print(\"n_stations:\", n_stations)\n",
    "\n",
    "model = CNNLSTM(\n",
    "    n_features=n_features,\n",
    "    n_stations=n_stations,\n",
    "    emb_dim=16,\n",
    "    conv_channels=32,\n",
    "    lstm_hidden=64,\n",
    "    lstm_layers=1,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"Embedding num_embeddings:\", model.station_emb.num_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e46ae2a-b471-4135-b396-a3d13865d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train MSE: 4.8679 | val RMSE: 1.9260 | val MAE: 0.8224 | val MAPE: 0.3387 | time: 41.5s\n",
      "  ✔ saved best model to /Users/zoltanjelovich/Documents/ISEG/MFW/data/modeling/station/cnn_sequences_v2/cnn_lstm_best_T24.pt (val RMSE=1.9260)\n",
      "Epoch 02 | train MSE: 3.1213 | val RMSE: 1.6476 | val MAE: 0.7274 | val MAPE: 0.3326 | time: 43.6s\n",
      "  ✔ saved best model to /Users/zoltanjelovich/Documents/ISEG/MFW/data/modeling/station/cnn_sequences_v2/cnn_lstm_best_T24.pt (val RMSE=1.6476)\n",
      "Epoch 03 | train MSE: 2.5728 | val RMSE: 1.5625 | val MAE: 0.7052 | val MAPE: 0.3376 | time: 46.1s\n",
      "  ✔ saved best model to /Users/zoltanjelovich/Documents/ISEG/MFW/data/modeling/station/cnn_sequences_v2/cnn_lstm_best_T24.pt (val RMSE=1.5625)\n",
      "Epoch 04 | train MSE: 2.3871 | val RMSE: 1.5532 | val MAE: 0.7045 | val MAPE: 0.3404 | time: 44.8s\n",
      "  ✔ saved best model to /Users/zoltanjelovich/Documents/ISEG/MFW/data/modeling/station/cnn_sequences_v2/cnn_lstm_best_T24.pt (val RMSE=1.5532)\n",
      "Epoch 05 | train MSE: 2.2114 | val RMSE: 1.5352 | val MAE: 0.7009 | val MAPE: 0.3436 | time: 44.3s\n",
      "  ✔ saved best model to /Users/zoltanjelovich/Documents/ISEG/MFW/data/modeling/station/cnn_sequences_v2/cnn_lstm_best_T24.pt (val RMSE=1.5352)\n",
      "Epoch 06 | train MSE: 2.1610 | val RMSE: 1.5664 | val MAE: 0.7182 | val MAPE: 0.3572 | time: 43.9s\n",
      "  no improvement (patience 1/2)\n",
      "Epoch 07 | train MSE: 2.0709 | val RMSE: 1.6049 | val MAE: 0.7319 | val MAPE: 0.3672 | time: 44.5s\n",
      "  no improvement (patience 2/2)\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# Helpers: iterate chunks + minibatches\n",
    "def list_chunks(seq_dir: Path, split: str, T: int):\n",
    "    return sorted(seq_dir.glob(f\"{split}_T{T}_chunk*.npz\"))\n",
    "\n",
    "def iter_minibatches_from_chunk(X, S, y, batch_size=1024, shuffle=True, seed=42):\n",
    "    n = X.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(idx)\n",
    "    for i in range(0, n, batch_size):\n",
    "        j = idx[i:i+batch_size]\n",
    "        yield X[j], S[j], y[j]\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_over_chunks(model, seq_dir: Path, split: str, T: int, mean_t, std_t,\n",
    "                     batch_size=2048, max_batches=200):\n",
    "    model.eval()\n",
    "    ys, yhats = [], []\n",
    "    n_batches = 0\n",
    "\n",
    "    for fp in list_chunks(seq_dir, split, T):\n",
    "        data = np.load(fp, allow_pickle=False)\n",
    "        X = torch.from_numpy(data[\"X\"]).to(torch.float32)\n",
    "        S = torch.from_numpy(data[\"S\"]).to(torch.int64)\n",
    "        y = torch.from_numpy(data[\"y\"]).to(torch.float32)\n",
    "\n",
    "        # normalize (broadcast over batch/time)\n",
    "        X = (X - mean_t) / std_t\n",
    "        X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # minibatch through this chunk\n",
    "        for xb, sb, yb in iter_minibatches_from_chunk(X.numpy(), S.numpy(), y.numpy(),\n",
    "                                                      batch_size=batch_size, shuffle=False):\n",
    "            xb = torch.from_numpy(xb).to(DEVICE)\n",
    "            sb = torch.from_numpy(sb).to(DEVICE)\n",
    "            yb = torch.from_numpy(yb).to(DEVICE)\n",
    "\n",
    "            yhat = model(xb, sb).detach().cpu().numpy()\n",
    "            ys.append(yb.detach().cpu().numpy())\n",
    "            yhats.append(yhat)\n",
    "\n",
    "            n_batches += 1\n",
    "            if max_batches is not None and n_batches >= max_batches:\n",
    "                y_true = np.concatenate(ys)\n",
    "                y_pred = np.concatenate(yhats)\n",
    "                return {\n",
    "                    \"rmse\": float(np.sqrt(np.mean((y_true - y_pred) ** 2))),\n",
    "                    \"mae\":  float(np.mean(np.abs(y_true - y_pred))),\n",
    "                    \"mape\": safe_mape(y_true, y_pred),\n",
    "                }\n",
    "\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(yhats)\n",
    "    return {\n",
    "        \"rmse\": float(np.sqrt(np.mean((y_true - y_pred) ** 2))),\n",
    "        \"mae\":  float(np.mean(np.abs(y_true - y_pred))),\n",
    "        \"mape\": safe_mape(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def train_one_epoch_over_chunks(model, seq_dir: Path, split: str, T: int, optimizer, criterion,\n",
    "                                mean_t, std_t, batch_size=1024, max_chunks=None, seed=42):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    chunk_files = list_chunks(seq_dir, split, T)\n",
    "    if max_chunks is not None:\n",
    "        chunk_files = chunk_files[:max_chunks]\n",
    "\n",
    "    for ci, fp in enumerate(chunk_files):\n",
    "        data = np.load(fp, allow_pickle=False)\n",
    "        X = torch.from_numpy(data[\"X\"]).to(torch.float32)\n",
    "        S = torch.from_numpy(data[\"S\"]).to(torch.int64)\n",
    "        y = torch.from_numpy(data[\"y\"]).to(torch.float32)\n",
    "\n",
    "        # Normalize\n",
    "        X = (X - mean_t) / std_t\n",
    "        X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # Minibatch loop\n",
    "        Xn, Sn, yn = X.numpy(), S.numpy(), y.numpy()\n",
    "        for xb, sb, yb in iter_minibatches_from_chunk(Xn, Sn, yn, batch_size=batch_size, shuffle=True, seed=seed+ci):\n",
    "            xb = torch.from_numpy(xb).to(DEVICE)\n",
    "            sb = torch.from_numpy(sb).to(DEVICE)\n",
    "            yb = torch.from_numpy(yb).to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb, sb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += float(loss.item()) * xb.shape[0]\n",
    "            total_n += xb.shape[0]\n",
    "\n",
    "    return total_loss / max(total_n, 1)\n",
    "\n",
    "# Training config\n",
    "T = 24\n",
    "EPOCHS = 10\n",
    "PATIENCE = 2\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "BEST_PATH = SEQ_DIR / f\"cnn_lstm_best_T{T}.pt\"\n",
    "\n",
    "mean_t = torch.from_numpy(mean).to(torch.float32).view(1, 1, -1).to(DEVICE)\n",
    "std_t  = torch.from_numpy(std).to(torch.float32).view(1, 1, -1).to(DEVICE)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "pat = 0\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_mse = train_one_epoch_over_chunks(\n",
    "        model, SEQ_DIR, \"train\", T,\n",
    "        optimizer, criterion,\n",
    "        mean_t, std_t,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_chunks=None\n",
    "    )\n",
    "\n",
    "    # Validation capped for speed; increase later if needed\n",
    "    val_metrics = eval_over_chunks(\n",
    "        model, SEQ_DIR, \"val\", T,\n",
    "        mean_t, std_t,\n",
    "        batch_size=2048,\n",
    "        max_batches=200\n",
    "    )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    val_rmse = val_metrics[\"rmse\"]\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | train MSE: {train_mse:.4f} | \"\n",
    "        f\"val RMSE: {val_rmse:.4f} | val MAE: {val_metrics['mae']:.4f} | val MAPE: {val_metrics['mape']:.4f} | \"\n",
    "        f\"time: {dt:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if val_rmse < best_val - 1e-4:\n",
    "        best_val = val_rmse\n",
    "        pat = 0\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\"  ✔ saved best model to {BEST_PATH} (val RMSE={best_val:.4f})\")\n",
    "    else:\n",
    "        pat += 1\n",
    "        print(f\"  no improvement (patience {pat}/{PATIENCE})\")\n",
    "        if pat >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30935772-44df-458e-8ef6-f9f18c50aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST CHECKPOINT METRICS (chunk-stream)\n",
      "VAL  | RMSE: 1.5351917743682861 | MAE: 0.7008955478668213 | MAPE: 0.34363049268722534\n",
      "TEST | RMSE: 1.9284042119979858 | MAE: 0.9354224801063538 | MAPE: 0.4100418984889984\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(BEST_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Mean/std tensors for broadcasting\n",
    "mean_t = torch.from_numpy(mean).to(torch.float32).view(1, 1, -1).to(DEVICE)\n",
    "std_t  = torch.from_numpy(std).to(torch.float32).view(1, 1, -1).to(DEVICE)\n",
    "\n",
    "# Fast evaluation over NPZ chunks (no DataLoader)\n",
    "val_metrics = eval_over_chunks(\n",
    "    model, SEQ_DIR, \"val\", T,\n",
    "    mean_t, std_t,\n",
    "    batch_size=2048,\n",
    "    max_batches=None\n",
    ")\n",
    "\n",
    "test_metrics = eval_over_chunks(\n",
    "    model, SEQ_DIR, \"test\", T,\n",
    "    mean_t, std_t,\n",
    "    batch_size=2048,\n",
    "    max_batches=None\n",
    ")\n",
    "\n",
    "print(\"\\nBEST CHECKPOINT METRICS (chunk-stream)\")\n",
    "print(\"VAL  | RMSE:\", val_metrics[\"rmse\"], \"| MAE:\", val_metrics[\"mae\"], \"| MAPE:\", val_metrics[\"mape\"])\n",
    "print(\"TEST | RMSE:\", test_metrics[\"rmse\"], \"| MAE:\", test_metrics[\"mae\"], \"| MAPE:\", test_metrics[\"mape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e18971ed-de22-4e20-a0a0-b7a95ee3c2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 3 | split=test\n",
      "\n",
      "PER-STATION METRICS (TEST) — summary\n",
      "Stations evaluated: 150\n",
      "Macro RMSE: 1.5267 | Macro MAE: 0.9354 | Macro MAPE: 89703.1261\n",
      "VolW  RMSE: 2.8461 | VolW  MAE: 1.8544 | VolW  MAPE: 131512.6515\n",
      "\n",
      "Worst 10 stations by RMSE (station_idx, n, rmse, mae, mape, volume):\n",
      "(1714, 1000, 6.255439021916436, 4.234659579992294, 103978.18137231923, 9875.0)\n",
      "(1510, 1000, 5.45482706787819, 3.168477591127157, 103789.68942926753, 5263.0)\n",
      "(1770, 1000, 4.87198933382591, 3.4219066592901943, 53292.52412310849, 8016.0)\n",
      "(1751, 1000, 4.773034124895573, 3.3179468105882406, 23641.816429834787, 5713.0)\n",
      "(1889, 1000, 4.548125371589033, 2.8835475091338156, 249490.4265081759, 4208.0)\n",
      "(1994, 1000, 4.30056075186176, 2.516526435859501, 153311.42828978968, 4891.0)\n",
      "(2114, 1000, 3.7248775045255487, 2.1847705082669853, 231406.28333466296, 3400.0)\n",
      "(1914, 1000, 3.711627642937558, 1.8489375579059124, 182562.04636686284, 2764.0)\n",
      "(2049, 1000, 3.688380854087593, 2.3307803369015456, 110070.29661660783, 4159.0)\n",
      "(2098, 1000, 3.5519179796672655, 2.4744941403865814, 194588.63549166286, 5539.0)\n"
     ]
    }
   ],
   "source": [
    "# Macro-average station metrics on TEST (from NPZ chunks)\n",
    "\n",
    "T = 24\n",
    "SPLIT = \"test\"\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# Ensure model + norm stats are loaded\n",
    "model.eval()\n",
    "\n",
    "def safe_mape_np(y_true, y_pred, eps=1e-6):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom))\n",
    "\n",
    "# Accumulators per station\n",
    "sum_sq_err = defaultdict(float)\n",
    "sum_abs_err = defaultdict(float)\n",
    "sum_abs_pct_err = defaultdict(float)\n",
    "sum_y = defaultdict(float)\n",
    "count = defaultdict(int)\n",
    "\n",
    "chunk_files = list_chunks(SEQ_DIR, SPLIT, T)\n",
    "print(f\"Chunks: {len(chunk_files)} | split={SPLIT}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for fp in chunk_files:\n",
    "        data = np.load(fp, allow_pickle=False)\n",
    "        X = torch.from_numpy(data[\"X\"]).to(torch.float32).to(DEVICE)\n",
    "        S = torch.from_numpy(data[\"S\"]).to(torch.int64).to(DEVICE)\n",
    "        y = torch.from_numpy(data[\"y\"]).to(torch.float32).to(DEVICE)\n",
    "\n",
    "        # normalize\n",
    "        X = (X - mean_t) / std_t\n",
    "        X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        for i in range(0, n, BATCH_SIZE):\n",
    "            xb = X[i:i+BATCH_SIZE]\n",
    "            sb = S[i:i+BATCH_SIZE]\n",
    "            yb = y[i:i+BATCH_SIZE]\n",
    "\n",
    "            yhat = model(xb, sb)\n",
    "\n",
    "            # Move to numpy for cheap aggregation\n",
    "            sb_np = sb.detach().cpu().numpy()\n",
    "            y_np = yb.detach().cpu().numpy()\n",
    "            yhat_np = yhat.detach().cpu().numpy()\n",
    "\n",
    "            err = y_np - yhat_np\n",
    "            abs_err = np.abs(err)\n",
    "            abs_pct_err = abs_err / np.maximum(np.abs(y_np), 1e-6)\n",
    "\n",
    "            for sid, e, ae, ape, yy in zip(sb_np, err, abs_err, abs_pct_err, y_np):\n",
    "                sum_sq_err[int(sid)] += float(e * e)\n",
    "                sum_abs_err[int(sid)] += float(ae)\n",
    "                sum_abs_pct_err[int(sid)] += float(ape)\n",
    "                sum_y[int(sid)] += float(yy)\n",
    "                count[int(sid)] += 1\n",
    "\n",
    "# Build per-station table\n",
    "station_ids = sorted(count.keys())\n",
    "per_station = []\n",
    "for sid in station_ids:\n",
    "    n = count[sid]\n",
    "    rmse = sqrt(sum_sq_err[sid] / n)\n",
    "    mae = sum_abs_err[sid] / n\n",
    "    mape = sum_abs_pct_err[sid] / n\n",
    "    vol = sum_y[sid]\n",
    "    per_station.append((sid, n, rmse, mae, mape, vol))\n",
    "\n",
    "# Macro averages\n",
    "macro_rmse = float(np.mean([r for _,_,r,_,_,_ in per_station]))\n",
    "macro_mae  = float(np.mean([m for _,_,_,m,_,_ in per_station]))\n",
    "macro_mape = float(np.mean([p for _,_,_,_,p,_ in per_station]))\n",
    "\n",
    "# Volume-weighted averages\n",
    "vols = np.array([v for *_, v in per_station], dtype=float)\n",
    "w = vols / np.maximum(vols.sum(), 1e-9)\n",
    "\n",
    "w_rmse = float(np.sum(w * np.array([r for _,_,r,_,_,_ in per_station])))\n",
    "w_mae  = float(np.sum(w * np.array([m for _,_,_,m,_,_ in per_station])))\n",
    "w_mape = float(np.sum(w * np.array([p for _,_,_,_,p,_ in per_station])))\n",
    "\n",
    "print(\"\\nPER-STATION METRICS (TEST) — summary\")\n",
    "print(\"Stations evaluated:\", len(per_station))\n",
    "print(f\"Macro RMSE: {macro_rmse:.4f} | Macro MAE: {macro_mae:.4f} | Macro MAPE: {macro_mape:.4f}\")\n",
    "print(f\"VolW  RMSE: {w_rmse:.4f} | VolW  MAE: {w_mae:.4f} | VolW  MAPE: {w_mape:.4f}\")\n",
    "\n",
    "# Inspect worst stations by RMSE (top 10)\n",
    "worst = sorted(per_station, key=lambda x: x[2], reverse=True)[:10]\n",
    "print(\"\\nWorst 10 stations by RMSE (station_idx, n, rmse, mae, mape, volume):\")\n",
    "for row in worst:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
